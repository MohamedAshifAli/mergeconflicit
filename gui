import ast
from glob import glob
from tkinter import filedialog
import customtkinter
import os
import tensorflow as tf
import pandas as pd
from keras.layers import BatchNormalization, Dropout, Flatten, Dense, Activation, MaxPooling2D, Conv2D, LSTM, \
    GlobalAveragePooling2D, MaxPool2D, multiply, Add, GlobalMaxPooling2D, Reshape,Lambda
from ReliefF import ReliefF
from keras.layers import Dropout, Flatten, BatchNormalization, Dense, Activation, Conv1D, Bidirectional, LSTM, LeakyReLU
from keras.models import load_model
import keras.utils
from scipy.stats import skew, kurtosis, entropy
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
import cv2
import numpy as np
from PIL import Image, ImageTk
from matplotlib import image as imm
from PIL import Image
from keras import Model, Input
from matplotlib import pyplot as plt
from keras.applications import VGG16, ResNet101
from sklearn.preprocessing import LabelEncoder
from termcolor import colored
from torchvision import models


def Explainable(image):
    print("Explainable CNN")
    image = cv2.cvtColor(np.uint8(image), cv2.COLOR_GRAY2BGR)## convert it into color
    Final = Guided_Propagation_Model(image, CAM_Model=GradCAMPlusPlus) ## GradCam++ alogrithm
    # Reshape Final to have 2154 rows and 3 columns
    Final_reshaped = Final.reshape(1, 2154, 3)

    # Check the shape of the reshaped array
    print(Final_reshaped.shape)  # Output: (1, 2154, 3)

    return Final_reshaped
    return Final

def Explainable_CNN_features(feat,option):
    print("Explainable CNN")
    if option==True:
        x_feat = []
        for i in range(feat.shape[0]):
            print(i)
            x_feat.append(Explainable(feat[i]))
        x_feat = np.vstack(x_feat)
    else:
        x_feat=np.load("Features/final_features.npy")
    return x_feat
# Define your CNN model
def Hybrid_Attention_Based_Explainable_CNN(xtrain,ytrain,xtest, ytest,epochs,option):
    xtrain = xtrain.reshape(xtrain.shape[0], xtrain.shape[1] // 2, 2, 1)
    xtest = xtest.reshape(xtest.shape[0], xtest.shape[1] // 2, 2, 1)
    model = keras.Sequential()
    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding="same",input_shape = xtrain.shape[1:]))
    model.add(MaxPooling2D(pool_size=(1,1)))
    model.add(BatchNormalization())

    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding="same"))
    model.add(MaxPooling2D(pool_size=(1,1)))
    model.add(BatchNormalization())

    model.add(Conv2D(96, kernel_size=(3, 3), activation='relu', padding="same"))
    model.add(MaxPooling2D(pool_size=(1,1)))
    model.add(BatchNormalization())

    model.add(Conv2D(96, kernel_size=(3, 3), activation='relu', padding="same"))
    model.add(MaxPooling2D(pool_size=(1,1)))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))

    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding="same"))
    model = Hybrid_Attention_Module(model)
    model.add(MaxPooling2D(pool_size=(1,1)))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))

    model.add(Flatten())
    model.add(Dense(256, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.3))
    model.add(Dense(ytrain.shape[1], activation='softmax')) ## No of classes
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    print('\033[46m' + '\033[30m' + "________________________Hybrid Attention Based Explainable CNN Model prepared...__________________________________" + '\x1b[0m')

    model.fit(xtrain, ytrain, batch_size=32, epochs=epochs, validation_split=0.2)
    if option == 0:
        model = model
    else:
        op = Optimization(model, xtest, ytest)
        model = op.main_update_hyperparameters(option)

    Y_pred = model.predict(xtest)
    Y_pred = np.argmax(Y_pred, axis=1)
    Y_true = np.argmax(ytest, axis=1)
    return Y_pred,Y_true
from mealpy.Prop import BaseProp as Prop
class Optimization:
    def __init__(self, model, x_test, y_test):
        self.model = model
        self.x_test = x_test
        self.y_test = y_test

    def fitness_function1(self, solution):
        print(colored("Fitness Function >> ", color='blue', on_color='on_grey'))
        wei_to_train = self.model.get_weights()
        wei_sh = wei_to_train[0]
        # wei = solution.reshape(wei_sh.shape[0], wei_sh.shape[1], wei_sh.shape[2], wei_sh.shape[3])
        wei_to_train[0] = wei_sh
        self.model.set_weights(wei_to_train)
        preds = self.model.predict(self.x_test)
        preds = np.argmax(preds, axis=1)
        y1_test = np.argmax(self.y_test, axis=1)
        acc = accuracy_score(y1_test, preds)
        return acc

    def main_weight_updation_optimization(self, curr_wei,option):
        problem_dict1 = {
            "fit_func": self.fitness_function1,
            "lb": [curr_wei.min(), ] * curr_wei.shape[0] * curr_wei.shape[1],
            "ub": [curr_wei.max(), ] * curr_wei.shape[0] * curr_wei.shape[1],
            "minmax": "max",
            "log_to": None,
            "save_population": False,
            "Curr_Weight": curr_wei,
        }
        print((colored("[INFO] Proposed Optimization \U0001F43A", 'magenta', on_color='on_grey')))
        model = Prop(problem_dict1, epoch=2, pop_size=10)
        best_position2, best_fitness2 = model.solve()
        return best_position2

    def main_update_hyperparameters(self, option):
        wei_to_train = self.model.get_weights()
        to_opt_1 = wei_to_train[0]
        re_to_opt_1 = to_opt_1.reshape(to_opt_1.shape[0] * to_opt_1.shape[1], to_opt_1.shape[2] * to_opt_1.shape[3])
        wei_to_train_1 = self.main_weight_updation_optimization(re_to_opt_1, option)
        to_opt_new = wei_to_train_1.reshape(to_opt_1.shape[0], to_opt_1.shape[1], to_opt_1.shape[2], to_opt_1.shape[3])
        wei_to_train[0] = to_opt_new
        self.model.set_weights(wei_to_train)
        return self.model

def get_pixel_of_mltp_left(image, center, x, y):
    new_value = 0
    try:
        if image[x][y] < center:
            new_value = 1 #-1
        elif image[x][y] == center:
            new_value = 0 # 0
        elif image[x][y] > center:
            new_value = 0  #1
    except:
        pass
    return new_value

def get_pixel_of_mltp_right(image, center, x, y):
    new_value = 0
    try:
        if image[x][y] < center:
            new_value = 0  #-1
        elif image[x][y] == center:
            new_value = 0 # 0
        elif image[x][y] > center:
            new_value = 1  #1
    except:
        pass
    return new_value

def calculated_pixel_of_mltp_right(image, x, y):
    val_ar = []
    center = image[x][y]
    # top_left
    val_ar.append(get_pixel_of_mltp_right(image, center, x - 1, y - 1))
    # top
    val_ar.append(get_pixel_of_mltp_right(image, center, x - 1, y))
    # top_right
    val_ar.append(get_pixel_of_mltp_right(image, center, x - 1, y + 1))
    # right
    val_ar.append(get_pixel_of_mltp_right(image, center, x, y + 1))
    # bottom_right
    val_ar.append(get_pixel_of_mltp_right(image, center, x + 1, y + 1))
    # bottom
    val_ar.append(get_pixel_of_mltp_right(image, center, x + 1, y))
    # bottom_left
    val_ar.append(get_pixel_of_mltp_right(image, center, x + 1, y - 1))
    # left
    val_ar.append(get_pixel_of_mltp_right(image, center, x, y - 1))
    # Now, we need to convert binary values to decimal
    power_val = [1, 2, 4, 8, 16, 32, 64, 128]
    val = 0
    for i in range(len(val_ar)):
        val += val_ar[i] * power_val[i]
    return val

def calculated_pixel_of_mltp_left(image, x, y):
    val_ar = []
    center = image[x][y]
    # top_left
    val_ar.append(get_pixel_of_mltp_left(image, center, x - 1, y - 1))
    # top
    val_ar.append(get_pixel_of_mltp_left(image, center, x - 1, y))
    # top_right
    val_ar.append(get_pixel_of_mltp_left(image, center, x - 1, y + 1))
    # right
    val_ar.append(get_pixel_of_mltp_left(image, center, x, y + 1))
    # bottom_right
    val_ar.append(get_pixel_of_mltp_left(image, center, x + 1, y + 1))
    # bottom
    val_ar.append(get_pixel_of_mltp_left(image, center, x + 1, y))
    # bottom_left
    val_ar.append(get_pixel_of_mltp_left(image, center, x + 1, y - 1))
    # left
    val_ar.append(get_pixel_of_mltp_left(image, center, x, y - 1))
    # Now, we need to convert binary values to decimal
    power_val = [1, 2, 4, 8, 16, 32, 64, 128]
    val = 0
    for i in range(len(val_ar)):
        val += val_ar[i] * power_val[i]
    return val
# from SRC.Functions import EarlyStoppingByLossVal
def Modified_ternary_texture(img):
    img = cv2. cvtColor(img, cv2.COLOR_BGR2GRAY)
    height, width = img.shape[0], img.shape[1]
    mltp_right = np.zeros((height, width), np.uint8)
    mltp_left = np.zeros((height, width), np.uint8)
    for i in range(0, height):
        for j in range(0, width):
            mltp_right[i, j] = calculated_pixel_of_mltp_right(img, i, j)
    for i in range(0, height):
        for j in range(0, width):
            mltp_left[i, j] = calculated_pixel_of_mltp_left(img, i, j)
    img_ltp = cv2.addWeighted(mltp_right, 0.5, mltp_left, 0.5, 0)
    return img_ltp

from Functions import getBounds, getWidth, getHeight, sharpen, oversample


#wnet segmentation

def position_attention(input_feature, ratio=8):
    # channel_axis = 1 if K.image_data_format() == "channels_first" else -1
    channel_axis = -1
    channel = input_feature.shape[channel_axis]

    shared_layer_one = Dense(channel // ratio,
                             activation='relu',
                             kernel_initializer='he_normal',
                             use_bias=True,
                             bias_initializer='zeros')
    shared_layer_two = Dense(channel,
                             kernel_initializer='he_normal',
                             use_bias=True,
                             bias_initializer='zeros')

    avg_pool = GlobalAveragePooling2D()(input_feature)
    avg_pool = Reshape((1, 1, channel))(avg_pool)
    assert avg_pool.shape[1:] == (1, 1, channel)
    avg_pool = shared_layer_one(avg_pool)
    assert avg_pool.shape[1:] == (1, 1, channel // ratio)
    avg_pool = shared_layer_two(avg_pool)
    assert avg_pool.shape[1:] == (1, 1, channel)

    max_pool = GlobalMaxPooling2D()(input_feature)
    max_pool = Reshape((1, 1, channel))(max_pool)
    assert max_pool.shape[1:] == (1, 1, channel)
    max_pool = shared_layer_one(max_pool)
    assert max_pool.shape[1:] == (1, 1, channel // ratio)
    max_pool = shared_layer_two(max_pool)
    assert max_pool.shape[1:] == (1, 1, channel)

    max_pool = GlobalMaxPooling2D()(input_feature)
    max_pool = Reshape((1, 1, channel))(max_pool)
    assert max_pool.shape[1:] == (1, 1, channel)
    max_pool = shared_layer_one(max_pool)
    assert max_pool.shape[1:] == (1, 1, channel // ratio)
    max_pool = shared_layer_two(max_pool)
    assert max_pool.shape[1:] == (1, 1, channel)

    pam_feature = Add()([avg_pool, max_pool])
    pam_feature = Activation('softmax')(pam_feature)

    return multiply([input_feature, pam_feature])
def ldp_process(photo):
    def assign_bit(picture, x, y, c1, c2, d):  # assign bit according to degree and neighbouring pixel
        # a and b are 1 if increasing and 0 if decreasing
        if d == 0:
            a = 0
            b = 0
            try:
                if picture[c1][c2 + 1] >= picture[c1][c2]:
                    a = 1
                if picture[x][y + 1] >= picture[x][y]:
                    b = 1
            except:
                pass
        if d == 45:
            a = 0
            b = 0
            try:
                if picture[c1 - 1][c2 + 1] >= picture[c1][c2]:
                    a = 1
                if picture[x - 1][y + 1] >= picture[x][y]:
                    b = 1
            except:
                pass
        if d == 90:
            a = 0
            b = 0
            try:
                if picture[c1 - 1][c2] >= picture[c1][c2]:
                    a = 1
                if picture[x - 1][y] >= picture[x][y]:
                    b = 1
            except:
                pass
        if d == 135:
            a = 0
            b = 0
            try:
                if picture[c1 - 1][c2 - 1] >= picture[c1][c2]:
                    a = 1
                if picture[x - 1][y - 1] >= picture[x][y]:
                    b = 1
            except:
                pass
        if a == b:  # if monotonically increasing or decreasing than 0
            return "0"
        else:  # if turning point
            return "1"

        return bit

    def local_der_val(picture, x, y):  # calculating local derivative pattern value of a pixel
        thirtytwo_bit_binary = []
        centre = picture[x][y]
        c1 = x
        c2 = y
        decimal_val = 0
        # starting from top left,assigning bit to pixels clockwise at 0 degree
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y - 1, c1, c2, 0))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y, c1, c2, 0))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y + 1, c1, c2, 0))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y + 1, c1, c2, 0))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y + 1, c1, c2, 0))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y, c1, c2, 0))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y - 1, c1, c2, 0))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y - 1, c1, c2, 0))

        # starting from top left,assigning bit to pixels clockwise at 45 degree
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y - 1, c1, c2, 45))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y, c1, c2, 45))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y + 1, c1, c2, 45))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y + 1, c1, c2, 45))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y + 1, c1, c2, 45))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y, c1, c2, 45))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y - 1, c1, c2, 45))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y - 1, c1, c2, 45))

        # starting from top left,assigning bit to pixels clockwise at 90 degree
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y - 1, c1, c2, 90))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y, c1, c2, 90))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y + 1, c1, c2, 90))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y + 1, c1, c2, 90))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y + 1, c1, c2, 90))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y, c1, c2, 90))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y - 1, c1, c2, 90))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y - 1, c1, c2, 90))

        # starting from top left,assigning bit to pixels clockwise at 135 degree
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y - 1, c1, c2, 135))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y, c1, c2, 135))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y + 1, c1, c2, 135))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y + 1, c1, c2, 135))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y + 1, c1, c2, 135))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y, c1, c2, 135))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y - 1, c1, c2, 135))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y - 1, c1, c2, 135))

        str1 = ""
        l = str1.join(thirtytwo_bit_binary)  # 32 bit binary number
        decimal_val = int(l, 2)  # 32 bit binary to decimal number
        return decimal_val

    m, n, _ = photo.shape
    # m, n, = photo.shape
    photo = cv2.cvtColor(photo, cv2.COLOR_BGR2GRAY)  # converting image to grayscale
    ldp_photo = np.zeros((m, n))
    # converting image to ldp
    for i in range(0, m):
        for j in range(0, n):
            ldp_photo[i, j] = local_der_val(photo, i, j)

    return ldp_photo
#canny shape
def edge_detection(img):
    a = (img * 255).astype("uint8")
    edges = cv2.Canny(a, 100, 200)  # Add threshold2 value (e.g., 200)
    return edges
def shape_structure_feautures(img):
    canny = edge_detection(img)
    ldp = ldp_process(img)
    combined_image = ldp + canny

    return combined_image


class ZeroChannelAttention:
    def __init__(self):
        self.avg_pool =  GlobalAveragePooling2D()
        self.max_pool = GlobalMaxPooling2D()

        self.sigmoid =Activation('sigmoid')

    def forward(self, x):
        self.avg_pool=self.avg_pool(x)
        self.max_pool=self.max_pool(x)
        self.x=Add()([self.avg_pool, self.max_pool])
        self.x=self.sigmoid(self.x)
        return self.x

class ZeroSpatialAttention:
    def __init__(self):
        self.sigmoid =Activation('sigmoid')

    def forward(self, X):
        self.avg_out =  Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(X)
        self.max_out= Lambda(lambda x: K.max(x, axis=3, keepdims=True))(X)
        self.x = Add()([self.avg_out, self.max_out])
        self.x = self.sigmoid(self.x)
        return self.x

class HybridAttention:
    def __init__(self, x,use_skip_connection=False):
        self.x=x
        self.ca = ZeroChannelAttention()
        self.sa = ZeroSpatialAttention()
        self.use_skip_connection = use_skip_connection

    def forward(self):
        out = self.x
        out = out + out * self.sa.forward(out) if self.use_skip_connection else out * self.sa.forward(out)
        out =out[:self.x.shape[0], :self.x.shape[1], :self.x.shape[2], :self.x.shape[3]]
        ## Hybrid
        out=np.array(out)
        out=position_attention(out)
        return out

def Hybrid_Attention_Block(w):
    import tensorflow as tf
    x = tf.convert_to_tensor(w)
    feature = HybridAttention(x).forward()
    newweight = tf.reshape(feature, [w.shape[0], w.shape[1], w.shape[2],w.shape[3]])
    newweight = np.array(newweight)
    return newweight

def Hybrid_Attention_Module(model):
    ###it will get thew weights frm module
    weights = model.get_weights()
    weight = weights[0]
    tunedweight = Hybrid_Attention_Block(weight)
    weights[0]=tunedweight
    model.set_weights(weights)
    return model
def Hybrid_Attention_Module_Segmentation(x):
    # Get Weights from previous layers
    all_x = x.node.layer.trainable_weights
    ## From all weights select zero th layer weights
    sel_x = all_x[0]
    w = np.array(sel_x)
    x = tf.convert_to_tensor(x)
    ## fed selected weights into Triplet attention Module
    weights = HybridAttention(x).forward()
    # Reshape according to old weight
    weights = weights.reshape(w.shape[0], w.shape[1],w.shape[2],w.shape[3])
    ## Convert it into tensor
    weights = tf.convert_to_tensor(weights)
    ## replace old weight by Triplet module generated weights
    all_x[0] = weights
    ## updated all weights in x
    x.node.layer.trainable_weights_ = all_x
    return x


def upconv_concat(bottom_a, bottom_b, n_filter, k_size, stride, padding='VALID'):
    up_conv = tf.layers.conv2d_transpose(bottom_a, filters=n_filter, kernel_size=[k_size, k_size],
                                         strides=stride, padding=padding)
    return tf.concat([up_conv, bottom_b], axis=-1)


def conv_layer(bottom, k_size, num_outputs, stride, padding='SAME'):
    input_channels = int(bottom.get_shape()[-1])
    weights = tf.Variable(tf.truncated_normal(shape=[k_size, k_size, input_channels, num_outputs], dtype=tf.float32,
                                              stddev=np.sqrt(1.0 / (k_size * k_size * input_channels))))
    biases = tf.Variable(tf.constant(0, dtype=tf.float32, shape=[num_outputs]))
    conv = tf.nn.conv2d(bottom, weights, strides=[1, stride, stride, 1], padding=padding)
    bias = tf.reshape(tf.nn.bias_add(conv, biases), tf.shape(conv))
    relu = tf.nn.relu(bias)

    return relu


def calc_loss(num_outputs, encoder_output, one_dim_kernel, sigma_pixel, spatial_kernel, batch_size):#lox calucltion
    num_sum = tf.constant(0.0, dtype=tf.float32)
    for depth in range(num_outputs):
        softmax_layer = encoder_output[:, :, :,
                        depth:depth + 1]  # take each channel of the output image from encoder_model
        extracted_pixels = tf.nn.conv2d(softmax_layer, one_dim_kernel, strides=[1, 1, 1, 1], padding='SAME')

        intensity_sq_dif = tf.squared_difference(extracted_pixels, softmax_layer)
        intensity_values = tf.exp(tf.divide(tf.negative(intensity_sq_dif), sigma_pixel))

        weights = tf.multiply(intensity_values, spatial_kernel)
        # Reshape Input Softmax Layer for correct dimensions
        u_pixels = tf.reshape(softmax_layer, [batch_size, 224, 224])

        # Calculate entire numerator
        numerator_inner_sum = tf.reduce_sum(tf.multiply(weights, extracted_pixels), axis=3)
        numerator_outer_sum = tf.multiply(u_pixels, numerator_inner_sum)
        numerator = tf.reduce_sum(numerator_outer_sum)

        # Calculate denominator
        denominator_inner_sum = tf.reduce_sum(weights, axis=3)
        denominator_outer_sum = tf.multiply(u_pixels, denominator_inner_sum)
        denominator = tf.reduce_sum(denominator_outer_sum)

        processed_value = numerator / denominator
        num_sum += processed_value

    return num_outputs - num_sum

def Hybrid_WNet_train(X_train):
    tf.disable_v2_behavior()
    # tf.compat.v1.disable_eager_execution()
    X_train = X_train.astype('float32') / 255.  # Standardizing the data
    X_train = np.reshape(X_train, (len(X_train), 224, 224, 1))  # reshape it to (1140, 224, 224, 1) for feeding to model

    height = width = 224
    channels = 1
    keep_prob = tf.placeholder_with_default(1.0, shape=())
    num_outputs = 3  # background, cell.
    num_epochs = 1500
    batch_size = 0
    r = 5
    sigma_dist = 4
    sigma_pixel = tf.square(tf.constant(10.0))
    input_img = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, height, width, channels], name='input_tensor')
    variance_epsilon = 0.0001
    with tf.name_scope('Encoder'):

        conv_1_1 = tf.layers.conv2d(input_img, 64, 3, padding='same', activation='relu')
        conv_1_1_with_attention = Hybrid_Attention_Module_Segmentation(conv_1_1)
        mean, variance = tf.nn.moments(conv_1_1_with_attention, [0, 1, 2])
        conv_1_1_bn = tf.nn.batch_normalization(conv_1_1, mean, variance, None, None, variance_epsilon)

        conv_1_1_drop = tf.layers.dropout(conv_1_1_bn, keep_prob)
        conv_1_2 = tf.layers.conv2d(conv_1_1_drop, 64, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_1_2, [0, 1, 2])
        conv_1_2_bn = tf.nn.batch_normalization(conv_1_2, mean, variance, None, None, variance_epsilon)
        conv_1_2_drop = tf.layers.dropout(conv_1_2_bn, keep_prob)

        pool_1 = tf.layers.max_pooling2d(conv_1_2_drop, pool_size=2, strides=2, padding="valid")

        conv_2_1 = tf.layers.separable_conv2d(pool_1, 128, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_2_1, [0, 1, 2])
        conv_2_1_bn = tf.nn.batch_normalization(conv_2_1, mean, variance, None, None, variance_epsilon)
        conv_2_1_drop = tf.layers.dropout(conv_2_1_bn, keep_prob)

        conv_2_2 = tf.layers.separable_conv2d(conv_2_1_drop, 128, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_2_2, [0, 1, 2])
        conv_2_2_bn = tf.nn.batch_normalization(conv_2_2, mean, variance, None, None, variance_epsilon)
        conv_2_2_drop = tf.layers.dropout(conv_2_2_bn, keep_prob)

        pool_2 = tf.layers.max_pooling2d(conv_2_2_drop, pool_size=2, strides=2, padding="valid")

        conv_3_1 = tf.layers.separable_conv2d(pool_2, 256, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_3_1, [0, 1, 2])
        conv_3_1_bn = tf.nn.batch_normalization(conv_3_1, mean, variance, None, None, variance_epsilon)
        conv_3_1_drop = tf.layers.dropout(conv_3_1_bn, keep_prob)

        conv_3_2 = tf.layers.separable_conv2d(conv_3_1_drop, 256, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_3_2, [0, 1, 2])
        conv_3_2_bn = tf.nn.batch_normalization(conv_3_2, mean, variance, None, None, variance_epsilon)
        conv_3_2_drop = tf.layers.dropout(conv_3_2_bn, keep_prob)

        pool_3 = tf.layers.max_pooling2d(conv_3_2_drop, pool_size=2, strides=2, padding="valid")

        conv_4_1 = tf.layers.separable_conv2d(pool_3, 512, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_4_1, [0, 1, 2])
        conv_4_1_bn = tf.nn.batch_normalization(conv_4_1, mean, variance, None, None, variance_epsilon)
        conv_4_1_drop = tf.layers.dropout(conv_4_1_bn, keep_prob)

        conv_4_2 = tf.layers.separable_conv2d(conv_4_1_drop, 512, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_4_2, [0, 1, 2])
        conv_4_2_bn = tf.nn.batch_normalization(conv_4_2, mean, variance, None, None, variance_epsilon)
        conv_4_2_drop = tf.layers.dropout(conv_4_2_bn, keep_prob)

        pool_4 = tf.layers.max_pooling2d(conv_4_2_drop, pool_size=2, strides=2, padding="valid")

        conv_5_1 = tf.layers.separable_conv2d(pool_4, 1024, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_5_1, [0, 1, 2])
        conv_5_1_bn = tf.nn.batch_normalization(conv_5_1, mean, variance, None, None, variance_epsilon)
        conv_5_1_drop = tf.layers.dropout(conv_5_1_bn, keep_prob)

        conv_5_2 = tf.layers.separable_conv2d(conv_5_1_drop, 1024, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_5_2, [0, 1, 2])
        conv_5_2_bn = tf.nn.batch_normalization(conv_5_2, mean, variance, None, None, variance_epsilon)
        conv_5_2_drop = tf.layers.dropout(conv_5_2_bn, keep_prob)
        upconv_1 = upconv_concat(conv_5_2_drop, conv_4_2_drop, n_filter=512, k_size=2, stride=2)

        conv_6_1 = tf.layers.separable_conv2d(upconv_1, 512, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_6_1, [0, 1, 2])
        conv_6_1_bn = tf.nn.batch_normalization(conv_6_1, mean, variance, None, None, variance_epsilon)
        conv_6_1_drop = tf.layers.dropout(conv_6_1_bn, keep_prob)

        conv_6_2 = tf.layers.separable_conv2d(conv_6_1_drop, 512, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_6_2, [0, 1, 2])
        conv_6_2_bn = tf.nn.batch_normalization(conv_6_2, mean, variance, None, None, variance_epsilon)
        conv_6_2_drop = tf.layers.dropout(conv_6_2_bn, keep_prob)

        upconv_2 = upconv_concat(conv_6_2_drop, conv_3_2_drop, n_filter=256, k_size=2, stride=2)

        conv_7_1 = tf.layers.separable_conv2d(upconv_2, 256, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_7_1, [0, 1, 2])
        conv_7_1_bn = tf.nn.batch_normalization(conv_7_1, mean, variance, None, None, variance_epsilon)
        conv_7_1_drop = tf.layers.dropout(conv_7_1_bn, keep_prob)

        conv_7_2 = tf.layers.separable_conv2d(conv_7_1_drop, 256, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_7_2, [0, 1, 2])
        conv_7_2_bn = tf.nn.batch_normalization(conv_7_2, mean, variance, None, None, variance_epsilon)
        conv_7_2_drop = tf.layers.dropout(conv_7_2_bn, keep_prob)

        upconv_3 = upconv_concat(conv_7_2_drop, conv_2_2_drop, n_filter=128, k_size=2, stride=2)

        conv_8_1 = tf.layers.separable_conv2d(upconv_3, 128, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_8_1, [0, 1, 2])
        conv_8_1_bn = tf.nn.batch_normalization(conv_8_1, mean, variance, None, None, variance_epsilon)
        conv_8_1_drop = tf.layers.dropout(conv_8_1_bn, keep_prob)

        conv_8_2 = tf.layers.separable_conv2d(conv_8_1_drop, 128, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_8_2, [0, 1, 2])
        conv_8_2_bn = tf.nn.batch_normalization(conv_8_2, mean, variance, None, None, variance_epsilon)
        conv_8_2_drop = tf.layers.dropout(conv_8_2_bn, keep_prob)

        upconv_4 = upconv_concat(conv_8_2_drop, conv_1_2_drop, n_filter=64, k_size=2, stride=2)

        conv_9_1 = tf.layers.separable_conv2d(upconv_4, 64, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_9_1, [0, 1, 2])
        conv_9_1_bn = tf.nn.batch_normalization(conv_9_1, mean, variance, None, None, variance_epsilon)
        conv_9_1_drop = tf.layers.dropout(conv_9_1_bn, keep_prob)

        conv_9_2 = tf.layers.separable_conv2d(conv_9_1_drop, 64, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_9_2, [0, 1, 2])
        conv_9_2_bn = tf.nn.batch_normalization(conv_9_2, mean, variance, None, None, variance_epsilon)
        conv_9_2_drop = tf.layers.dropout(conv_9_2_bn, keep_prob)

        conv = tf.layers.conv2d(conv_9_2_drop, num_outputs, kernel_size=1, strides=1, padding='same', activation='relu')
        print(conv.shape, "conv")
        encoder_output = tf.nn.softmax(conv, axis=3, name="output_tensor")

        init = tf.global_variables_initializer()

        s = 2 * r + 1
        spatial_kernel = np.zeros((s, s), dtype=np.float32)
        for y in range(s):
            for x in range(s):
                # calculate squared euclidean distance
                dist = (x - r) * (x - r) + (y - r) * (y - r)
                if dist < (r * r):
                    spatial_kernel[y][x] = np.exp((-dist) / sigma_dist)

        spatial_kernel = tf.constant(spatial_kernel.reshape(-1), dtype=tf.float32)

        # create one dimensional kernel

        s = 2 * r + 1
        one_dim_kernel = np.zeros((s, s, (s * s)))
        for i in range(s * s):
            one_dim_kernel[int(i / s)][i % s][i] = 1.0
        one_dim_kernel = one_dim_kernel.reshape(s, s, 1, (s * s))
        one_dim_kernel = tf.constant(one_dim_kernel, dtype=tf.float32)

        loss = calc_loss(num_outputs, encoder_output, one_dim_kernel, sigma_pixel, spatial_kernel, batch_size)
        soft_cut_norm_loss = tf.reduce_mean(loss)
        with tf.name_scope("optimizer"):
            norm_cut_opt = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(soft_cut_norm_loss)  # optimizer

        init = tf.global_variables_initializer()

        with tf.Session() as sess:
            sess.run(init)
        for epoch in range(num_epochs):
            print("epoch:", epoch)
        count = 0
        batch_start_index = 0
        while (count != 1):
            X_train_batch = X_train[batch_start_index: batch_start_index + batch_size]
            _, train_loss = sess.run([norm_cut_opt, soft_cut_norm_loss],
                                     feed_dict={input_img: X_train_batch, keep_prob: 0.7})
            batch_start_index += batch_size
            count += 1
            print("Train loss after epoch ", str(epoch), "is", str(train_loss))
        #     saved_path = saver.save(sess, './my-model')
        output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(),
                                                                        ["Encoder/output_tensor"])
        with tf.gfile.GFile("wnetmodel.pb", "wb") as f:
            f.write(output_graph_def.SerializeToString())

    with tf.Session() as sess:
        sess.run(init)
        #     saver.restore(sess, './my-model')
        output = sess.run(encoder_output, feed_dict={input_img: X_train})
    return output

def get_pixel(image, center, x, y):
    new_value = 0
    try:
        if image[x][y] > center:
            new_value = 1
    except:
        pass
    return new_value


def lbp_calculated_pixel(image, x, y):
    center = image[x][y]
    val_ar = []

    # top_left
    val_ar.append(get_pixel(image, center, x - 1, y - 1))

    # top
    val_ar.append(get_pixel(image, center, x - 1, y))

    # top_right
    val_ar.append(get_pixel(image, center, x - 1, y + 1))

    # right
    val_ar.append(get_pixel(image, center, x, y + 1))

    # bottom_right
    val_ar.append(get_pixel(image, center, x + 1, y + 1))

    # bottom
    val_ar.append(get_pixel(image, center, x + 1, y))

    # bottom_left
    val_ar.append(get_pixel(image, center, x + 1, y - 1))

    # left
    val_ar.append(get_pixel(image, center, x, y - 1))

    # Now, we need to convert binary values to decimal
    power_val = [1, 2, 4, 8, 16, 32, 64, 128]
    val = 0
    for i in range(len(val_ar)):
        val += val_ar[i] * power_val[i]

    return val


def Local_Binary_Pattern(image):
    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    width, height = image.shape
    img_lbp = np.zeros((height, width), np.uint8)
    for i in range(0, height):
        for j in range(0, width):
            img_lbp[i, j] = lbp_calculated_pixel(image, i, j)
    return img_lbp

def Hybrid_Color_based_Structural_Pattern(image, Resize):
    ## Apply Local Binary Pattern algorithm in given image and get structural output
    structural_image = Local_Binary_Pattern(image)
    if Resize:
        structural_image = cv2.resize(structural_image, (28, 28))
    ## Apply  four different color maps in the structural image
    colormap1 = cv2.applyColorMap(structural_image, cv2.COLORMAP_HOT)
    colormap2 = cv2.applyColorMap(structural_image, cv2.COLORMAP_BONE)
    colormap3 = cv2.applyColorMap(structural_image, cv2.COLORMAP_PINK)
    colormap4 = cv2.applyColorMap(structural_image, cv2.COLORMAP_OCEAN)
    ### Add all colormaps into one
    hybrid = colormap1 + colormap2 + colormap3 + colormap4
    return hybrid

def f(x):
    return ast.literal_eval(x.rstrip('\r\n'))

def sharpen(img):

    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])
    sharpened_image = cv2.filter2D(img, -1, kernel)
    return sharpened_image

def preprocessing_object_detection(image):
    Features = []
    label = []
    df = pd.read_csv("Datasets/annotations.csv",
                     converters={'geometry': eval})
    # Create bounds, width and height
    df.loc[:, 'bounds'] = df.loc[:, 'geometry'].apply(getBounds)
    df.loc[:, 'width'] = df.loc[:, 'bounds'].apply(getWidth)
    df.loc[:, 'height'] = df.loc[:, 'bounds'].apply(getHeight)

    image_files = glob('Datasets/ALlimages/*.*')
    unique, counts = np.unique(df['class'], return_counts=True)

    filename = os.path.basename(image_files[image])
    all_indexes = np.where(df['image_id'] == filename)[0]
    img = cv2.imread(image_files[image])

    cnt = 1
    for ind in all_indexes:
        print("Preprocessing : " + str(cnt))
        image = img.copy()
        label.append(df['class'][ind])
        coordinates = df['bounds'][ind]
        xB = coordinates[2]
        xA = coordinates[0]
        yB = coordinates[3]
        yA = coordinates[1]
        cv2.rectangle(image, (xA, yA), (xB, yB), (0, 0, 0), 0)
        orig_filename = 'Rectangular rounded Objects\\img__' + '_orig.jpg'
        cv2.imwrite(orig_filename, image)
        cropped_img = image[yA:yB, xA:xB]
        sharpened_img = sharpen(cropped_img)
        dst = cv2.fastNlMeansDenoisingColored(sharpened_img, None, 5, 5, 7, 21)
        resized = cv2.resize(dst, (224, 224), interpolation=cv2.INTER_AREA)
        filename = 'Detected Objects\\img__' + str(cnt) + '.jpg'
        cv2.imwrite(filename, resized)
        cnt += 1
        Features.append(resized)  # append to list


    feat = np.asarray(Features)  # convert to numpy array
    lab = np.asarray(label)

    return feat

def Segmentation(feat):
    img = np.load(feat[0])
    # images = images[:, :, :, 0]
    all_outputs = []
    print("\033[93mSegmentation\033[0m : " )
    output =Hybrid_WNet_train(img)
    all_outputs.append(output)
    all_outputs = np.asarray(output)
    np.save("segmentatione.npy", all_outputs)

class App(customtkinter.CTk):
    def __init__(self):
        super().__init__()

        self.ii = None
        self.check = None
        self.modell = None
        self.show = None
        self.image2 = None
        self.image51 = None
        self.image9 = None
        self.YPred = None
        self.model = None
        self.test = None
        self.image1 = None
        self.Home_8_inside = None
        self.Home_12_inside = None
        self.xtest3 = None
        self.Home_9_inside = None
        self.xtest1 = None
        self.Home_3_inside = None
        self.hybrid_image = None
        self.ff = None
        self.image = None
        self.Home_1_inside = None
        self.filename3 = None
        self.filename2 = None
        self.filename1 = None
        self.A = None
        self.ind = None
        self.index = None
        self.frame_4_inside_button4 = None
        self.frame_3_inside_button3 = None
        self.frame_3_inside_button2 = None
        self.frame_4_inside_button3 = None
        self.frame_3_inside_button4 = None
        self.frame_2_inside_button3 = None
        self.frame_5_inside_button1 = None
        self.frame_4_inside_button5 = None
        self.frame_5_inside_button2 = None
        self.frame_4_inside_button2 = None
        self.frame_4_inside_button1 = None
        self.frame_3_inside_button1 = None
        self.frame_2_inside_button4 = None
        self.frame_2_inside_button2 = None
        self.frame_2_inside_button1 = None
        self.filename = None
        self.title("Object Detection in Remote Sensing")
        self.geometry("800x800")

        # set grid layout 1x2
        self.grid_rowconfigure(0, weight=1)
        self.grid_columnconfigure(1, weight=1)

        # load images with light and dark mode image
        image_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), "Demo_FileImages")
        self.logo_image = customtkinter.CTkImage(Image.open(os.path.join(image_path, "logo_single.jpg")), size=(26, 26))
        self.large_test_image = customtkinter.CTkImage(Image.open(os.path.join(image_path, "image1.jpg")),
                                                       size=(500, 150))
        self.image_icon_image = customtkinter.CTkImage(Image.open(os.path.join(image_path, "image_icon_light.png")),
                                                       size=(20, 20))
        self.home_image = customtkinter.CTkImage(light_image=Image.open(os.path.join(image_path, "home_dark.png")),
                                                 dark_image=Image.open(os.path.join(image_path, "home_light.png")),
                                                 size=(20, 20))
        self.chat_image = customtkinter.CTkImage(light_image=Image.open(os.path.join(image_path, "chat_dark.png")),
                                                 dark_image=Image.open(os.path.join(image_path, "chat_light.png")),
                                                 size=(20, 20))
        self.add_user_image = customtkinter.CTkImage(
            light_image=Image.open(os.path.join(image_path, "add_user_dark.png")),
            dark_image=Image.open(os.path.join(image_path, "add_user_light.png")), size=(20, 20))

        # create navigation frame
        self.navigation_frame = customtkinter.CTkFrame(self, corner_radius=0)
        self.navigation_frame.grid(row=0, column=0, sticky="nsew")
        self.navigation_frame.grid_rowconfigure(6, weight=1)

        self.navigation_frame_label = customtkinter.CTkLabel(self.navigation_frame, text="Plant Disease Detection",
                                                             compound="left",
                                                             font=customtkinter.CTkFont(size=15, weight="bold"))
        self.navigation_frame_label.grid(row=0, column=0, padx=20, pady=20)

        self.home_button = customtkinter.CTkButton(self.navigation_frame, corner_radius=0, height=40, border_spacing=10,
                                                   text="Select Image",
                                                   fg_color="transparent", text_color=("gray10", "gray90"),
                                                   hover_color=("gray70", "gray30"),
                                                   anchor="w", command=self.frame_1_inside_button1_event,
                                                   font=customtkinter.CTkFont(size=12, weight="bold"))
        self.home_button.grid(row=1, column=0, sticky="ew")
        self.frame_3_button = customtkinter.CTkButton(self.navigation_frame, corner_radius=0, height=40,
                                                      border_spacing=10, text="Feature Extraction",
                                                      fg_color="transparent", text_color=("gray10", "gray90"),
                                                      hover_color=("gray70", "gray30"), anchor="w",
                                                      command=self.frame_2_inside_button1_event,
                                                      font=customtkinter.CTkFont(size=12, weight="bold"))
        self.frame_3_button.grid(row=2, column=0, sticky="ew")
        self.frame_4_button = customtkinter.CTkButton(self.navigation_frame, corner_radius=0, height=40,
                                                      border_spacing=10, text="Detect",
                                                      fg_color="transparent", text_color=("gray10", "gray90"),
                                                      hover_color=("gray70", "gray30"), anchor="w",
                                                      command=self.frame_3_inside_button1_event,
                                                      font=customtkinter.CTkFont(size=12, weight="bold"))
        self.frame_4_button.grid(row=3, column=0, sticky="ew")

        self.frame_5_button = customtkinter.CTkButton(self.navigation_frame, corner_radius=0, height=40,
                                                      border_spacing=10, text="Refresh",
                                                      fg_color="transparent", text_color=("gray10", "gray90"),
                                                      hover_color=("gray70", "gray30"), anchor="w",
                                                      command=self.frame_4_inside_button1_event,
                                                      font=customtkinter.CTkFont(size=12, weight="bold"))
        self.frame_5_button.grid(row=4, column=0, sticky="ew")

        self.frame_6_button = customtkinter.CTkButton(self.navigation_frame, corner_radius=0, height=40,
                                                      border_spacing=10, text="Exit",
                                                      fg_color="transparent", text_color=("gray10", "gray90"),
                                                      hover_color=("gray70", "gray30"), anchor="w",
                                                      command=self.frame_5_inside_button1_event,
                                                      font=customtkinter.CTkFont(size=12, weight="bold"))
        self.frame_6_button.grid(row=5, column=0, sticky="ew")

        self.appearance_mode_menu = customtkinter.set_appearance_mode("dark")
        customtkinter.set_default_color_theme("blue")

        # create home frame
        self.home_frame = customtkinter.CTkFrame(self, corner_radius=0, fg_color="transparent")
        self.home_frame.grid_columnconfigure(0, weight=1)


    def f(self):
        self.YPred = self.model.predict(self.test)
        self.YPred = np.argmax(self.YPred,axis=1)
        self.YPred = self.YPred[0]

    def proposed_model(self,xtest,db):
        xtest=xtest.astype("float32")/xtest.max()
        xtest = xtest.reshape(xtest.shape[0], xtest.shape[1] // 3, 3,1)
        self.model=load_model("Model\\"+db+"\\Model.h5")
        self.test = xtest
        self.f()


    def select_frame_by_name(self, name):
        # set button color for selected button
        self.home_button.configure(fg_color=("gray75", "gray25") if name == "home" else "transparent")
        self.frame_4_button.configure(fg_color=("gray75", "gray25") if name == "frame_4" else "transparent")

        # show selected frame
        if name == "home":
            self.home_frame.grid(row=0, column=1, sticky="nsew")
        else:
            self.home_frame.grid_forget()

    def home_button_event(self):
        self.select_frame_by_name("home")


    def change_appearance_mode_event(self, new_appearance_mode):
        customtkinter.set_appearance_mode(new_appearance_mode)

    def frame_1_inside_button1_event(self):
        global img
        f_types = [('All Files', '*.*')]
        self.filename = filedialog.askopenfilename(initialdir="Dataset", filetypes=f_types)
        self.check = os.path.split(os.path.split(os.path.split(self.filename)[0])[0])[1]
        self.select_frame_by_name("home")
        sharpen()
        self.image = cv2.imread(self.filename)
        self.image=cv2.cvtColor(self.image,cv2.COLOR_BGR2RGB)
        sharpened_image = sharpen(self.image)
        image1 = Image.fromarray(sharpened_image)
        self.show= customtkinter.CTkImage(image1,size=(200, 200))
        self.Home_1_inside = customtkinter.CTkLabel(self.home_frame, text="Enhanced Image", compound='bottom',
                                                     image=self.show,font=customtkinter.CTkFont(size=12, weight="bold"))
        self.Home_1_inside.grid(row=0, column=0, padx=10, pady=10)

    def frame_2_inside_button1_event(self):
        img_p=preprocessing_object_detection(self.filename)
        Wnet_segmnted = Hybrid_WNet_train(img_p)
        np.save("segmentatione.npy", Wnet_segmnted)
        imm.imsave("GUIImages\\Wnet_segmnted.jpg",Wnet_segmnted)
        image1 = Image.open("GUIImages/Wnet_segmnted.jpg")
        self.show = customtkinter.CTkImage(image1, size=(200, 200))
        self.Home_2_inside = customtkinter.CTkLabel(self.home_frame, text="Preprocessed Image", compound='bottom',
                                                    image=self.show, font=customtkinter.CTkFont(size=12, weight="bold"))
        self.Home_2_inside_.grid(row=0, column=1, padx=10, pady=10)

    from PIL import Image, ImageTk
    import numpy as np

    def frame_3_inside_button1_event(self):
        segmented_image = Image.open("GUIImages/Wnet_segmnted.jpg")
        segmented_image_np = np.array(segmented_image)

        self.image = cv2.resize(segmented_image_np, (200, 200))
        ldp_image = ldp_process(self.image)

        ldp_show = Image.fromarray(ldp_image)
        ldp_ctk_image = customtkinter.CTkImage(ldp_show, size=(200, 200))

        # Display the LDP processed image
        self.Home_9_inside = customtkinter.CTkLabel(self.home_frame, text="LDP Processed Image", compound='bottom',
                                                    image=ldp_ctk_image,
                                                    font=customtkinter.CTkFont(size=12, weight="bold"))
        self.Home_9_inside.grid(row=2, column=1, padx=10, pady=10)

        # Call edge detection function
        canny_image = edge_detection(self.image)
        canny_show = Image.fromarray(canny_image)
        canny_ctk_image = customtkinter.CTkImage(canny_show, size=(200, 200))

        # Display the Canny edge image
        self.Home_10_inside = customtkinter.CTkLabel(self.home_frame, text="Canny Edge Image", compound='bottom',
                                                     image=canny_ctk_image,
                                                     font=customtkinter.CTkFont(size=12, weight="bold"))
        self.Home_10_inside.grid(row=2, column=2, padx=10, pady=10)

        # Combine the LDP processed image and Canny edge image
        combined_image = ldp_image + canny_image

        # Convert the combined image to be compatible with tkinter
        combined_show = Image.fromarray(combined_image)
        combined_ctk_image = customtkinter.CTkImage(combined_show, size=(200, 200))

        self.Home_11_inside = customtkinter.CTkLabel(self.home_frame, text="Combined Image", compound='bottom',
                                                     image=combined_ctk_image,
                                                     font=customtkinter.CTkFont(size=12, weight="bold"))
        self.Home_11_inside.grid(row=2, column=3, padx=10, pady=10)
        modified_texture_image = Modified_ternary_texture(segmented_image_np)
        modified_texture_show = Image.fromarray(modified_texture_image)
        modified_texture_ctk_image = customtkinter.CTkImage(modified_texture_show, size=(200, 200))
        self.Home_12_inside = customtkinter.CTkLabel(self.home_frame, text="Modified Ternary Texture Processed Image",
                                                     compound='bottom',
                                                     image=modified_texture_ctk_image,
                                                     font=customtkinter.CTkFont(size=12, weight="bold"))
        self.Home_12_inside.grid(row=2, column=4, padx=10, pady=10)

    def frame_4_inside_button1_event(self):
        image_path = 'Rectangular rounded Objects\\img__orig.jpg'
        labels = np.load("Final Featres/Labels.npy")
        image = cv2.imread(image_path)
        features = Explainable_CNN_features(image, option=True)
        le = LabelEncoder()

        labels = le.fit_transform(labels)
        features, labels = oversample(features, labels)
        prediction = Hybrid_Attention_Based_Explainable_CNN(features,labels)
        result_text = "Truncated" if prediction == 1 else "Non-truncated"
        cv2.putText(image, result_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        modified_texture_show = Image.fromarray(image)
        modified_texture_tk = ImageTk.PhotoImage(modified_texture_show)
        self.Home_13_inside = customtkinter.CTkLabel(self.home_frame, text="Modified Ternary Texture Processed Image",
                                                     compound='bottom',
                                                     image=modified_texture_tk,
                                                     font=customtkinter.CTkFont(size=12, weight="bold"))
        self.Home_13_inside.image = modified_texture_tk  # Keep a reference to prevent garbage collection
        self.Home_13_inside.grid(row=2, column=4, padx=10, pady=10)

        self.home_frame.grid_forget()

    def frame_5_inside_button1_event(self):
        app.destroy()

if __name__ == "__main__":
    app = App()
    app.mainloop()
