import math
import os

import keras
import pandas as pd
import smote
from imblearn.over_sampling import RandomOverSampler
from keras import Model, Sequential
from keras.applications import ResNet101
from keras.layers import BatchNormalization, Dropout, Flatten, Dense, Activation, MaxPooling2D, Conv2D, LSTM, \
    GlobalAveragePooling2D, MaxPool2D
from keras.utils import to_categorical
from matplotlib import image as mpimg
from skimage import color
from skimage.feature import graycoprops, graycomatrix
from skimage.util import img_as_ubyte
from glob import glob
import cv2
import matplotlib.pyplot as plt
import numpy as np
from numpy.random import seed
import ast
from glob import glob
import cv2
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
from skimage.feature import graycomatrix, graycoprops
from sklearn.metrics import multilabel_confusion_matrix
from termcolor import colored

seed(1)
import tensorflow
tensorflow.random.set_seed(2)
import tensorflow as tf
from keras.layers import BatchNormalization
from glob import glob
import cv2
import matplotlib.pyplot as plt
import tensorflow.compat.v1 as tf
import warnings
warnings.filterwarnings('ignore',category= UserWarning)
from tensorflow.keras.applications import ResNet101
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dropout, Flatten, BatchNormalization, Dense, Activation

tf.disable_v2_behavior()

def f(x):
    return ast.literal_eval(x.rstrip('\r\n')) #convrt the annotations strings into their original data types integr for the usage in nprogram


def getBounds(geometry):#gettg the calculated bounding box coordinates xmin, ymin, xmax, ymax.

    try:
        arr = np.array(geometry).T
        xmin = np.min(arr[0])
        ymin = np.min(arr[1])
        xmax = np.max(arr[0])
        ymax = np.max(arr[1])
        return (xmin, ymin, xmax, ymax)
    except:
        return np.nan


def getWidth(bounds): #gettg widh
    try:
        (xmin, ymin, xmax, ymax) = bounds
        return np.abs(xmax - xmin)
    except:
        return np.nan


def getHeight(bounds):#gettg height points from annottaions
    try:
        (xmin, ymin, xmax, ymax) = bounds
        return np.abs(ymax - ymin)
    except:
        return np.nan


def upconv_concat(bottom_a, bottom_b, n_filter, k_size, stride, padding='VALID'):
    up_conv = tf.layers.conv2d_transpose(bottom_a, filters=n_filter, kernel_size=[k_size, k_size],
                                         strides=stride, padding=padding)
    return tf.concat([up_conv, bottom_b], axis=-1)


def conv_layer(bottom, k_size, num_outputs, stride, padding='SAME'):
    input_channels = int(bottom.get_shape()[-1])
    weights = tf.Variable(tf.truncated_normal(shape=[k_size, k_size, input_channels, num_outputs], dtype=tf.float32,
                                              stddev=np.sqrt(1.0 / (k_size * k_size * input_channels))))
    biases = tf.Variable(tf.constant(0, dtype=tf.float32, shape=[num_outputs]))
    conv = tf.nn.conv2d(bottom, weights, strides=[1, stride, stride, 1], padding=padding)
    bias = tf.reshape(tf.nn.bias_add(conv, biases), tf.shape(conv))
    relu = tf.nn.relu(bias)

    return relu


def calc_loss(num_outputs, encoder_output, one_dim_kernel, sigma_pixel, spatial_kernel, batch_size):#lox calucltion
    num_sum = tf.constant(0.0, dtype=tf.float32)
    for depth in range(num_outputs):
        softmax_layer = encoder_output[:, :, :,
                        depth:depth + 1]  # take each channel of the output image from encoder_model
        extracted_pixels = tf.nn.conv2d(softmax_layer, one_dim_kernel, strides=[1, 1, 1, 1], padding='SAME')

        intensity_sq_dif = tf.squared_difference(extracted_pixels, softmax_layer)
        intensity_values = tf.exp(tf.divide(tf.negative(intensity_sq_dif), sigma_pixel))

        weights = tf.multiply(intensity_values, spatial_kernel)
        # Reshape Input Softmax Layer for correct dimensions
        u_pixels = tf.reshape(softmax_layer, [batch_size, 224, 224])

        # Calculate entire numerator
        numerator_inner_sum = tf.reduce_sum(tf.multiply(weights, extracted_pixels), axis=3)
        numerator_outer_sum = tf.multiply(u_pixels, numerator_inner_sum)
        numerator = tf.reduce_sum(numerator_outer_sum)

        # Calculate denominator
        denominator_inner_sum = tf.reduce_sum(weights, axis=3)
        denominator_outer_sum = tf.multiply(u_pixels, denominator_inner_sum)
        denominator = tf.reduce_sum(denominator_outer_sum)

        processed_value = numerator / denominator
        num_sum += processed_value

    return num_outputs - num_sum


def WNet_train(X_train):
    X_train = X_train.astype('float32') / 255.  # Standardizing the data
    X_train = np.reshape(X_train, (len(X_train), 224, 224, 1))  # reshape it to (1140, 224, 224, 1) for feeding to model

    height = width = 224
    channels = 1
    keep_prob = tf.placeholder_with_default(1.0, shape=())
    num_outputs = 3  # background, cell.
    num_epochs = 1500
    batch_size = 0
    r = 5
    sigma_dist = 4
    sigma_pixel = tf.square(tf.constant(10.0))
    input_img = tf.placeholder(dtype=tf.float32, shape=[None, height, width, channels], name='input_tensor')
    variance_epsilon = 0.0001
    with tf.name_scope('Encoder'):
        conv_1_1 = tf.layers.conv2d(input_img, 64, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_1_1, [0, 1, 2])
        # conv_1_1_bn = Batch_Normalization(conv_1_1, mean, variance, None, None, variance_epsilon)
        conv_1_1_bn = BatchNormalization(epsilon=variance_epsilon)(conv_1_1)

        conv_1_1_drop = tf.layers.dropout(conv_1_1_bn, keep_prob)
        conv_1_2 = tf.layers.conv2d(conv_1_1_drop, 64, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_1_2, [0, 1, 2])
        conv_1_2_bn = tf.nn.batch_normalization(conv_1_2, mean, variance, None, None, variance_epsilon)
        conv_1_2_drop = tf.layers.dropout(conv_1_2_bn, keep_prob)

        pool_1 = tf.layers.max_pooling2d(conv_1_2_drop, pool_size=2, strides=2, padding="valid")

        conv_2_1 = tf.layers.separable_conv2d(pool_1, 128, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_2_1, [0, 1, 2])
        conv_2_1_bn = tf.nn.batch_normalization(conv_2_1, mean, variance, None, None, variance_epsilon)
        conv_2_1_drop = tf.layers.dropout(conv_2_1_bn, keep_prob)

        conv_2_2 = tf.layers.separable_conv2d(conv_2_1_drop, 128, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_2_2, [0, 1, 2])
        conv_2_2_bn = tf.nn.batch_normalization(conv_2_2, mean, variance, None, None, variance_epsilon)
        conv_2_2_drop = tf.layers.dropout(conv_2_2_bn, keep_prob)

        pool_2 = tf.layers.max_pooling2d(conv_2_2_drop, pool_size=2, strides=2, padding="valid")

        conv_3_1 = tf.layers.separable_conv2d(pool_2, 256, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_3_1, [0, 1, 2])
        conv_3_1_bn = tf.nn.batch_normalization(conv_3_1, mean, variance, None, None, variance_epsilon)
        conv_3_1_drop = tf.layers.dropout(conv_3_1_bn, keep_prob)

        conv_3_2 = tf.layers.separable_conv2d(conv_3_1_drop, 256, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_3_2, [0, 1, 2])
        conv_3_2_bn = tf.nn.batch_normalization(conv_3_2, mean, variance, None, None, variance_epsilon)
        conv_3_2_drop = tf.layers.dropout(conv_3_2_bn, keep_prob)

        pool_3 = tf.layers.max_pooling2d(conv_3_2_drop, pool_size=2, strides=2, padding="valid")

        conv_4_1 = tf.layers.separable_conv2d(pool_3, 512, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_4_1, [0, 1, 2])
        conv_4_1_bn = tf.nn.batch_normalization(conv_4_1, mean, variance, None, None, variance_epsilon)
        conv_4_1_drop = tf.layers.dropout(conv_4_1_bn, keep_prob)

        conv_4_2 = tf.layers.separable_conv2d(conv_4_1_drop, 512, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_4_2, [0, 1, 2])
        conv_4_2_bn = tf.nn.batch_normalization(conv_4_2, mean, variance, None, None, variance_epsilon)
        conv_4_2_drop = tf.layers.dropout(conv_4_2_bn, keep_prob)

        pool_4 = tf.layers.max_pooling2d(conv_4_2_drop, pool_size=2, strides=2, padding="valid")

        conv_5_1 = tf.layers.separable_conv2d(pool_4, 1024, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_5_1, [0, 1, 2])
        conv_5_1_bn = tf.nn.batch_normalization(conv_5_1, mean, variance, None, None, variance_epsilon)
        conv_5_1_drop = tf.layers.dropout(conv_5_1_bn, keep_prob)

        conv_5_2 = tf.layers.separable_conv2d(conv_5_1_drop, 1024, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_5_2, [0, 1, 2])
        conv_5_2_bn = tf.nn.batch_normalization(conv_5_2, mean, variance, None, None, variance_epsilon)
        conv_5_2_drop = tf.layers.dropout(conv_5_2_bn, keep_prob)

        upconv_1 = upconv_concat(conv_5_2_drop, conv_4_2_drop, n_filter=512, k_size=2, stride=2)

        conv_6_1 = tf.layers.separable_conv2d(upconv_1, 512, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_6_1, [0, 1, 2])
        conv_6_1_bn = tf.nn.batch_normalization(conv_6_1, mean, variance, None, None, variance_epsilon)
        conv_6_1_drop = tf.layers.dropout(conv_6_1_bn, keep_prob)

        conv_6_2 = tf.layers.separable_conv2d(conv_6_1_drop, 512, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_6_2, [0, 1, 2])
        conv_6_2_bn = tf.nn.batch_normalization(conv_6_2, mean, variance, None, None, variance_epsilon)
        conv_6_2_drop = tf.layers.dropout(conv_6_2_bn, keep_prob)

        upconv_2 = upconv_concat(conv_6_2_drop, conv_3_2_drop, n_filter=256, k_size=2, stride=2)

        conv_7_1 = tf.layers.separable_conv2d(upconv_2, 256, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_7_1, [0, 1, 2])
        conv_7_1_bn = tf.nn.batch_normalization(conv_7_1, mean, variance, None, None, variance_epsilon)
        conv_7_1_drop = tf.layers.dropout(conv_7_1_bn, keep_prob)

        conv_7_2 = tf.layers.separable_conv2d(conv_7_1_drop, 256, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_7_2, [0, 1, 2])
        conv_7_2_bn = tf.nn.batch_normalization(conv_7_2, mean, variance, None, None, variance_epsilon)
        conv_7_2_drop = tf.layers.dropout(conv_7_2_bn, keep_prob)

        upconv_3 = upconv_concat(conv_7_2_drop, conv_2_2_drop, n_filter=128, k_size=2, stride=2)

        conv_8_1 = tf.layers.separable_conv2d(upconv_3, 128, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_8_1, [0, 1, 2])
        conv_8_1_bn = tf.nn.batch_normalization(conv_8_1, mean, variance, None, None, variance_epsilon)
        conv_8_1_drop = tf.layers.dropout(conv_8_1_bn, keep_prob)

        conv_8_2 = tf.layers.separable_conv2d(conv_8_1_drop, 128, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_8_2, [0, 1, 2])
        conv_8_2_bn = tf.nn.batch_normalization(conv_8_2, mean, variance, None, None, variance_epsilon)
        conv_8_2_drop = tf.layers.dropout(conv_8_2_bn, keep_prob)

        upconv_4 = upconv_concat(conv_8_2_drop, conv_1_2_drop, n_filter=64, k_size=2, stride=2)

        conv_9_1 = tf.layers.separable_conv2d(upconv_4, 64, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_9_1, [0, 1, 2])
        conv_9_1_bn = tf.nn.batch_normalization(conv_9_1, mean, variance, None, None, variance_epsilon)
        conv_9_1_drop = tf.layers.dropout(conv_9_1_bn, keep_prob)

        conv_9_2 = tf.layers.separable_conv2d(conv_9_1_drop, 64, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_9_2, [0, 1, 2])
        conv_9_2_bn = tf.nn.batch_normalization(conv_9_2, mean, variance, None, None, variance_epsilon)
        conv_9_2_drop = tf.layers.dropout(conv_9_2_bn, keep_prob)

        conv = tf.layers.conv2d(conv_9_2_drop, num_outputs, kernel_size=1, strides=1, padding='same', activation='relu')
        print(conv.shape, "conv")
        encoder_output = tf.nn.softmax(conv, axis=3, name="output_tensor")

    init = tf.global_variables_initializer()

    s = 2 * r + 1
    spatial_kernel = np.zeros((s, s), dtype=np.float32)
    for y in range(s):
        for x in range(s):
            # calculate squared euclidean distance
            dist = (x - r) * (x - r) + (y - r) * (y - r)
            if dist < (r * r):
                spatial_kernel[y][x] = np.exp((-dist) / sigma_dist)

    spatial_kernel = tf.constant(spatial_kernel.reshape(-1), dtype=tf.float32)

    # create one dimensional kernel

    s = 2 * r + 1
    one_dim_kernel = np.zeros((s, s, (s * s)))
    for i in range(s * s):
        one_dim_kernel[int(i / s)][i % s][i] = 1.0
    one_dim_kernel = one_dim_kernel.reshape(s, s, 1, (s * s))
    one_dim_kernel = tf.constant(one_dim_kernel, dtype=tf.float32)

    loss = calc_loss(num_outputs, encoder_output, one_dim_kernel, sigma_pixel, spatial_kernel, batch_size)
    soft_cut_norm_loss = tf.reduce_mean(loss)
    with tf.name_scope("optimizer"):
        norm_cut_opt = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(soft_cut_norm_loss)  # optimizer

    init = tf.global_variables_initializer()

    with tf.Session() as sess:
        sess.run(init)
        for epoch in range(num_epochs):
            print("epoch:", epoch)
            count = 0
            batch_start_index = 0
            while (count != 1):
                X_train_batch = X_train[batch_start_index: batch_start_index + batch_size]
                _, train_loss = sess.run([norm_cut_opt, soft_cut_norm_loss],
                                         feed_dict={input_img: X_train_batch, keep_prob: 0.7})
                batch_start_index += batch_size
                count += 1
            print("Train loss after epoch ", str(epoch), "is", str(train_loss))
        #     saved_path = saver.save(sess, './my-model')
        output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(),
                                                                        ["Encoder/output_tensor"])
        with tf.gfile.GFile("wnetmodel.pb", "wb") as f:
            f.write(output_graph_def.SerializeToString())

    with tf.Session() as sess:
        sess.run(init)
        #     saver.restore(sess, './my-model')
        output = sess.run(encoder_output, feed_dict={input_img: X_train})
    return output

def sharpen(img):

    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])
    sharpened_image = cv2.filter2D(img, -1, kernel)
    return sharpened_image

def preprocessing_object_detection():
    Features = []
    label = []
    df = pd.read_csv("Datasets/annotations.csv",
                     converters={'geometry': f})
    # Create bounds, width and height
    df.loc[:, 'bounds'] = df.loc[:, 'geometry'].apply(getBounds)
    df.loc[:, 'width'] = df.loc[:, 'bounds'].apply(getWidth)
    df.loc[:, 'height'] = df.loc[:, 'bounds'].apply(getHeight)

    image_files = glob('Datasets/ALlimages/*.*')
    unique, counts = np.unique(df['class'], return_counts=True)
    cnt = 1
    for image in image_files:
        filename = os.path.basename(image)
        all_indexes = np.where(df['image_id'] == filename)[0]
        img = cv2.imread(image)
        ### Image Enhancement
        image = img
        for ind in all_indexes:
            print("Preprocessing : "+str(cnt))
            img=image.copy()
            label.append(df['class'][ind])
            coordinates = df['bounds'][ind]
            xB = coordinates[2]
            xA = coordinates[0]
            yB = coordinates[3]
            yA = coordinates[1]
            cv2.rectangle(img, (xA, yA), (xB, yB), (0, 0, 0), 0)
            img = img[yA:yB, xA:xB]
            getSharpen = sharpen(img)
            dst = cv2.fastNlMeansDenoisingColored(getSharpen, None, 5, 5, 7, 21)
            resized = cv2.resize(dst, (224, 224), interpolation=cv2.INTER_AREA)
            filename = 'Detected Objects\\img__' + str(cnt) + '.jpg'
            cv2.imwrite(filename, resized)
            cnt += 1
            # Resize image to (224, 224)
            Features.append(resized)  # append to list
    feat=np.asarray(Features)  # convert to numpy array
    lab = np.asarray(label)
    np.save("Features/Images.npy",feat)
    np.save("Features/Labels.npy",lab)

# preprocessing_object_detection()

def preprocessing_feature_extraction():
    images = np.load("Features/Images.npy")
    images = images[:, :, :, 0]
    all_outputs = []
    cntt=0
    batch_size = 50
    for i in range(0, len(images), batch_size):
        print("\033[93mSegmentation\033[0m : " + str(cntt))
        end_index = min(i + batch_size, len(images))
        output = WNet_train(images[i:end_index])
        cntt +=1
        for k in output:
            all_outputs.append(k)
            plt.imshow(k)
    all_outputss = np.asarray(all_outputs)
    np.save("segmentatione.npy", all_outputs)
# preprocessing_feature_extraction()

def entropy(glcm):
    entropy1 = -np.sum(glcm * np.log2(glcm + (glcm == 0)))
    return entropy1
# //////////shape and Structural Feauture Extrcation

def ldp_process(photo):
    def assign_bit(picture, x, y, c1, c2, d):  # assign bit according to degree and neighbouring pixel
        # a and b are 1 if increasing and 0 if decreasing
        if d == 0:
            a = 0
            b = 0
            try:
                if picture[c1][c2 + 1] >= picture[c1][c2]:
                    a = 1
                if picture[x][y + 1] >= picture[x][y]:
                    b = 1
            except:
                pass
        if d == 45:
            a = 0
            b = 0
            try:
                if picture[c1 - 1][c2 + 1] >= picture[c1][c2]:
                    a = 1
                if picture[x - 1][y + 1] >= picture[x][y]:
                    b = 1
            except:
                pass
        if d == 90:
            a = 0
            b = 0
            try:
                if picture[c1 - 1][c2] >= picture[c1][c2]:
                    a = 1
                if picture[x - 1][y] >= picture[x][y]:
                    b = 1
            except:
                pass
        if d == 135:
            a = 0
            b = 0
            try:
                if picture[c1 - 1][c2 - 1] >= picture[c1][c2]:
                    a = 1
                if picture[x - 1][y - 1] >= picture[x][y]:
                    b = 1
            except:
                pass
        if a == b:  # if monotonically increasing or decreasing than 0
            return "0"
        else:  # if turning point
            return "1"

        return bit

    def local_der_val(picture, x, y):  # calculating local derivative pattern value of a pixel
        thirtytwo_bit_binary = []
        centre = picture[x][y]
        c1 = x
        c2 = y
        decimal_val = 0
        # starting from top left,assigning bit to pixels clockwise at 0 degree
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y - 1, c1, c2, 0))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y, c1, c2, 0))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y + 1, c1, c2, 0))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y + 1, c1, c2, 0))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y + 1, c1, c2, 0))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y, c1, c2, 0))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y - 1, c1, c2, 0))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y - 1, c1, c2, 0))

        # starting from top left,assigning bit to pixels clockwise at 45 degree
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y - 1, c1, c2, 45))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y, c1, c2, 45))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y + 1, c1, c2, 45))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y + 1, c1, c2, 45))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y + 1, c1, c2, 45))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y, c1, c2, 45))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y - 1, c1, c2, 45))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y - 1, c1, c2, 45))

        # starting from top left,assigning bit to pixels clockwise at 90 degree
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y - 1, c1, c2, 90))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y, c1, c2, 90))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y + 1, c1, c2, 90))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y + 1, c1, c2, 90))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y + 1, c1, c2, 90))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y, c1, c2, 90))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y - 1, c1, c2, 90))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y - 1, c1, c2, 90))

        # starting from top left,assigning bit to pixels clockwise at 135 degree
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y - 1, c1, c2, 135))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y, c1, c2, 135))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y + 1, c1, c2, 135))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y + 1, c1, c2, 135))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y + 1, c1, c2, 135))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y, c1, c2, 135))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y - 1, c1, c2, 135))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y - 1, c1, c2, 135))

        str1 = ""
        l = str1.join(thirtytwo_bit_binary)  # 32 bit binary number
        decimal_val = int(l, 2)  # 32 bit binary to decimal number
        return decimal_val

    m, n, _ = photo.shape
    # m, n, = photo.shape
    photo = cv2.cvtColor(photo, cv2.COLOR_BGR2GRAY)  # converting image to grayscale
    ldp_photo = np.zeros((m, n))
    # converting image to ldp
    for i in range(0, m):
        for j in range(0, n):
            ldp_photo[i, j] = local_der_val(photo, i, j)

    return ldp_photo
#canny shape
def edge_detection(img):
    a = (img * 255).astype("uint8")
    edges = cv2.Canny(a, 100, 200)  # Add threshold2 value (e.g., 200)
    return edges
def shape_structure_feautures(img):
    canny = edge_detection(img)
    ldp = ldp_process(img)
    combined_image = ldp + canny

    return combined_image

def main_resnet_feat_ext(base_model):
    x = base_model.output
    x = Dropout(0.05)(x)
    x = Flatten()(x)
    x = BatchNormalization()(x)
    x = Dense(1024, kernel_initializer='he_uniform')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dropout(0.05)(x)
    x = Dense(1024, kernel_initializer='he_uniform')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dropout(0.05)(x)
    x = Dense(1024, kernel_initializer='he_uniform')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dropout(0.05)(x)
    predictions = Dense(100, activation='softmax')(x)
    model_feat = Model(inputs=base_model.input, outputs=predictions)
    return model_feat
def feat_resnet(array, input_shape):
    base_model = ResNet101(include_top=False, weights='imagenet', input_shape=input_shape)
    Model_feat = main_resnet_feat_ext(base_model)
    feat = Model_feat.predict(array)
    return feat
def Resnet101_Feature_Extraction(image1):
    """Residual Network (ResNet) is a deep learning model used for computer vision applications.
    It is a Convolutional Neural Network (CNN) architecture designed to support hundreds or thousands of convolutional layers
    Every layer of a ResNet is composed of several blocks. This is because when ResNets go deeper,
    they normally do it by increasing the number of operations within a block,
    but the number of total layers remains the same.ResNet-101 is a convolutional neural network that is 101 layers deep.
    You can load a pretrained version of the network trained on more than a million images from the ImageNet database.
    he pretrained network can classify images into 1000 object categories"""
    input_shape = (image1.shape[1], image1.shape[2], image1.shape[3])
    resnet_feat = feat_resnet(image1,input_shape)  # resnet feature extraction
    return resnet_feat

#LTP PAttern
def get_pixel_of_mltp_left(image, center, x, y):
    new_value = 0
    try:
        if image[x][y] < center:
            new_value = 1 #-1
        elif image[x][y] == center:
            new_value = 0 # 0
        elif image[x][y] > center:
            new_value = 0  #1
    except:
        pass
    return new_value

def get_pixel_of_mltp_right(image, center, x, y):
    new_value = 0
    try:
        if image[x][y] < center:
            new_value = 0  #-1
        elif image[x][y] == center:
            new_value = 0 # 0
        elif image[x][y] > center:
            new_value = 1  #1
    except:
        pass
    return new_value

def calculated_pixel_of_mltp_right(image, x, y):
    val_ar = []
    center = image[x][y]
    # top_left
    val_ar.append(get_pixel_of_mltp_right(image, center, x - 1, y - 1))
    # top
    val_ar.append(get_pixel_of_mltp_right(image, center, x - 1, y))
    # top_right
    val_ar.append(get_pixel_of_mltp_right(image, center, x - 1, y + 1))
    # right
    val_ar.append(get_pixel_of_mltp_right(image, center, x, y + 1))
    # bottom_right
    val_ar.append(get_pixel_of_mltp_right(image, center, x + 1, y + 1))
    # bottom
    val_ar.append(get_pixel_of_mltp_right(image, center, x + 1, y))
    # bottom_left
    val_ar.append(get_pixel_of_mltp_right(image, center, x + 1, y - 1))
    # left
    val_ar.append(get_pixel_of_mltp_right(image, center, x, y - 1))
    # Now, we need to convert binary values to decimal
    power_val = [1, 2, 4, 8, 16, 32, 64, 128]
    val = 0
    for i in range(len(val_ar)):
        val += val_ar[i] * power_val[i]
    return val

def calculated_pixel_of_mltp_left(image, x, y):
    val_ar = []
    center = image[x][y]
    # top_left
    val_ar.append(get_pixel_of_mltp_left(image, center, x - 1, y - 1))
    # top
    val_ar.append(get_pixel_of_mltp_left(image, center, x - 1, y))
    # top_right
    val_ar.append(get_pixel_of_mltp_left(image, center, x - 1, y + 1))
    # right
    val_ar.append(get_pixel_of_mltp_left(image, center, x, y + 1))
    # bottom_right
    val_ar.append(get_pixel_of_mltp_left(image, center, x + 1, y + 1))
    # bottom
    val_ar.append(get_pixel_of_mltp_left(image, center, x + 1, y))
    # bottom_left
    val_ar.append(get_pixel_of_mltp_left(image, center, x + 1, y - 1))
    # left
    val_ar.append(get_pixel_of_mltp_left(image, center, x, y - 1))
    # Now, we need to convert binary values to decimal
    power_val = [1, 2, 4, 8, 16, 32, 64, 128]
    val = 0
    for i in range(len(val_ar)):
        val += val_ar[i] * power_val[i]
    return val
#LTP PAttern
def Modified_ternary_texture(img):
    img = cv2. cvtColor(img, cv2.COLOR_BGR2GRAY)
    height, width = img.shape[0], img.shape[1]
    mltp_right = np.zeros((height, width), np.uint8)
    mltp_left = np.zeros((height, width), np.uint8)
    for i in range(0, height):
        for j in range(0, width):
            mltp_right[i, j] = calculated_pixel_of_mltp_right(img, i, j)
    for i in range(0, height):
        for j in range(0, width):
            mltp_left[i, j] = calculated_pixel_of_mltp_left(img, i, j)
    img_ltp = cv2.addWeighted(mltp_right, 0.5, mltp_left, 0.5, 0)
    return img_ltp

#GLCM Features

def GLCM_Feature_Extraction(img):
    rgbImg = img
    grayImg = img_as_ubyte(color.rgb2gray(rgbImg))
    distances = [1, 2, 3]
    angles = [0, np.pi / 4, np.pi / 2, 3 * np.pi / 4]
    glcm = graycomatrix(grayImg, distances=distances, angles=angles, symmetric=True, normed=True)
    dis = (graycoprops(glcm, 'dissimilarity'))
    dis = np.mean(dis)
    eng = (graycoprops(glcm, 'energy'))
    eng = np.mean(eng)
    hom = (graycoprops(glcm, 'homogeneity'))
    hom = np.mean(hom)
    cor = (graycoprops(glcm, 'correlation'))
    cor = np.mean(cor)
    con = (graycoprops(glcm, 'contrast'))
    con = np.mean(con)
    en = entropy(glcm)
    feat = np.hstack((dis, eng, hom, cor, con, en))
    return feat




def Feature_Extraction():
    images = np.load("segmentatione.npy")
    features = []
    reimg = []

    cnt = 0
    for i in range(len(images)):
        print(colored("Feature Extraction :  >> "+ str(cnt), color='blue'))
        img = cv2.resize(images[i], (32, 32))
        im1 = Modified_ternary_texture(img)
        im3 = GLCM_Feature_Extraction(img)
        im4 = shape_structure_feautures(img)
        im4 = im4.reshape(1, im4.shape[0] * im4.shape[1])
        im1 = im1.reshape(1, im1.shape[0] * im1.shape[1])
        im3 = im3.reshape(1, im3.shape[0])
        im = np.hstack((im1,im3, im4))  # Concatenate all features
        im4 = img.reshape(1,img.shape[0],img.shape[1],img.shape[2])
        features.append(im)
        reimg.append(im4)
        cnt+=1

    feat1 = np.vstack(features)
    arra = np.vstack(reimg)
    feat2 = Resnet101_Feature_Extraction(arra)
    final_features = np.hstack((feat1, feat2))
    np.save("Features/final_features.npy", final_features)

from sklearn.preprocessing import LabelEncoder
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
# from keras.layers.recurrent import LSTM
# from tensorflow.keras.layers import LSTM
from keras.models import load_model
import keras
import os
from keras.layers import  Embedding, LSTM,Bidirectional,SimpleRNN,Conv1D
from keras import metrics, regularizers
from keras.optimizers import SGD,RMSprop, Adam, Adadelta, Adagrad ,Adamax, Nadam
import seaborn as sns
from keras.utils import np_utils
def resize_features(features):
    new_height = 224
    new_width = 224
    resized_features = []
    for feature in features:
        resized_image = cv2.resize(feature, (new_width, new_height))
        var = np.zeros((new_height, new_width, 3))
        var[:, :, 0] = resized_image
        resized_features.append(var)
    return np.array(resized_features)

def split_data(features, labels,  train_size):
    total_data = len(features)
    indices = np.arange(total_data)
    np.random.shuffle(indices)

    split_index = int(total_data * (1 - train_size))
    train_indices = indices[:split_index]
    test_indices = indices[split_index:]

    xtrain = features[train_indices]
    xtest = features[test_indices]
    ytrain = labels[train_indices]
    ytest = labels[test_indices]

    return xtrain, xtest, ytrain, ytest

from keras import layers
# def oversample(X_train, y_train):
#     ros = RandomOverSampler(random_state=42)
#     X_resampled, y_resampled = ros.fit_resample(X_train, y_train)
#     return X_resampled, y_resampled

from sklearn.preprocessing import MinMaxScaler
# #COMPARITIVE MODEL -1def LSTM_Model(xtrain, ytrain,xtest, ytest,epochs):
# def LSTM_Model(xtrain, ytrain, xtest, ytest):
#
#     model = Sequential()
#     model.add(Conv2D(32, (3, 3), padding="same", activation="relu", input_shape=(None, 150, 150, 3)))
#     # model.add(LSTM(32, return_sequences=True))
#     model.add(MaxPool2D((2, 2), strides=2))
#
#     model.add(Conv2D(32, (3, 3), padding="same", activation="relu"))
#     # model.add(LSTM(64, return_sequences=True))
#     model.add(MaxPool2D((2, 2), strides=2))
#
#     model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=0.001, rho=0.9, epsilon=1e-6), metrics=['acc'])
#
#     print('Model prepared...')
#
#     model.fit(xtrain, ytrain, batch_size=32, epochs=15, validation_split=0.2)
#
#     Y_pred = model.predict(xtest)
#     Y_pred = np.argmax(Y_pred, axis=1)
#     Y_true = np.argmax(ytest, axis=1)
#
#     return Y_pred, Y_true

def LSTM_Model(xtrain, ytrain, xtest, ytest):
    xtrain = xtrain.reshape(xtrain.shape[0], xtrain.shape[1] // 2, 2, 1)
    xtest = xtest.reshape(xtest.shape[0], xtest.shape[1] // 2, 2, 1)

    model = Sequential()
    model.add(Conv2D(32, (3, 3), padding="same", activation="relu", input_shape = xtrain.shape[1:]))
    model.add(MaxPooling2D((2, 2)))
    # model.add(LSTM(32, return_sequences=True))
    # model.add(LSTM(64, return_sequences=True))
    # model.add(LSTM(128))
    # model.add(Dropout(0.2))
    # model.add(Flatten())
    model.add(Dropout(0.5))
    model.add(Dense(ytrain.shape[1], activation="softmax"))

    model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=0.001, rho=0.9, epsilon=1e-6), metrics=['acc'])

    print('Model prepared...')

    model.fit(xtrain, ytrain, batch_size=32, epochs=10, validation_split=0.2)

    Y_pred = model.predict(xtest)
    Y_pred = np.argmax(Y_pred, axis=1)
    Y_true = np.argmax(ytest, axis=1)

    return Y_pred, Y_true


def Bi_LSTM_MODEL(xtrain, ytrain, xtest, ytest):
    epochs = 10
    xtrain = xtrain.reshape(xtrain.shape[0], xtrain.shape[1] // 2, 2 ,1 )
    xtest = xtest.reshape(xtest.shape[0], xtest.shape[1] // 2, 2 ,1)

    model = Sequential()
    model.add(Bidirectional(LSTM(128, activation='relu', return_sequences=False, input_shape = xtrain.shape[1:])))
    model.add(Dropout(0.2))
    model.add(Dense(ytrain.shape[1], activation='softmax'))

    model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=0.001, rho=0.9, epsilon=1e-6), metrics=['acc'])

    print('Model prepared...')

    model.fit(xtrain, ytrain, batch_size=32, epochs=epochs, validation_split=0.2)

    Y_pred = model.predict(xtest)
    Y_pred = np.argmax(Y_pred, axis=1)
    Y_true = np.argmax(ytest, axis=1)

    return Y_pred, Y_true


def CNN_Based_On_ResNet(xtrain, ytrain, xtest, ytest):
    epochs = 5
    xtrain = resize_features(xtrain)
    xtest = resize_features(xtest)

    base_layer = tensorflow.keras.applications.ResNet50(
        include_top=False,
        weights="imagenet",
        input_tensor=None,
        input_shape=(224, 224, 3),
        pooling=None,
    )

    model = Sequential([
        base_layer,
        layers.Flatten(),
        layers.Dense(32, activation="relu"),
        layers.BatchNormalization(),
        layers.Dense(ytrain.shape[1], activation="softmax") ])

    model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=0.001, rho=0.9, epsilon=1e-6), metrics=['acc'])

    print('Model prepared...')

    model.fit(xtrain, ytrain, batch_size=32, epochs=epochs, validation_split=0.2)

    Y_pred = model.predict(xtest)
    Y_pred = np.argmax(Y_pred, axis=1)
    Y_true = np.argmax(ytest, axis=1)

    return Y_pred, Y_true

#//////////////Rnn

#//RNN pending
# def RNN(xtrain, ytrain,xtest, ytest,epochs):
#     epochs = 10
#     xtrain = xtrain.reshape(xtrain.shape[0], 28, 28)
#     xtest = xtest.reshape(xtest.shape[0], 28, 28)
#
#     model = tf.keras.Sequential([
#         tf.keras.Input(shape=(28, 28)),
#         tf.keras.layers.GRU(128),
#         tf.keras.layers.Dense(128, activation='relu', input_shape=(28, 28,)),
#         tf.keras.layers.Dropout(0.2, input_shape=(128,)),
#         tf.keras.layers.Dense(10, activation='softmax')
#     ])
#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
#     print('Model prepared...')
#     model.fit(xtrain, ytrain, batch_size=32, epochs=epochs, validation_split=0.2)
#     Y_pred = model.predict(xtest)
#     Y_pred = np.argmax(Y_pred, axis=1)
#     Y_true = np.argmax(ytest, axis=1)
#
#     return Y_pred, Y_true
# #newCNN
# def EfficientNetB3(xtrain, ytrain, xtest, ytest, epochs):
#     y2train = ytrain.reshape(-1, 1)  # Reshape to (num_samples, 1)
#
#     y2train = to_categorical(ytrain)  # Convert to one-hot encoded format
#       # Convert to one-hot encoded format
#
#     model_main = tf.keras.applications.efficientnet.EfficientNetB3(input_shape=(224, 224, 3), include_top=False,
#                                                                    weights='imagenet')
#     model_main.trainable = False
#     model = tf.keras.Sequential([
#         model_main,
#         GlobalAveragePooling2D(name="avg_pool"),
#         BatchNormalization(),
#         Dropout(0.2, name="top_dropout"),
#         Dense(256, activation='relu', activity_regularizer=tf.keras.regularizers.l2(1e-5)),
#         Dropout(0.25),
#         Dense(y2train.shape[1], activation="softmax")
#     ])
#
#     model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=0.001, rho=0.9, epsilon=1e-6), metrics=['acc'])
#
#     print('Model prepared...')
#
#     model.fit(xtrain, ytrain, batch_size=32, epochs=epochs, validation_split=0.2)
#
#     Y_pred = model.predict(xtest)
#     Y_pred = np.argmax(Y_pred, axis=1)
#     Y_true = np.argmax(ytest, axis=1)
#
#     return Y_pred, Y_true
#



# Define your CNN model
def Hybrid_Attention_Based_Explainable_CNN(xtrain, ytrain,xtest, ytest,epochs):

    xtrain = xtrain.reshape(xtrain.shape[0], xtrain.shape[1] // 2, 2, 1)
    xtest = xtest.reshape(xtest.shape[0], xtest.shape[1] // 2, 2, 1)


    model = keras.Sequential()
    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding="same",input_shape = xtrain.shape[1:]))
    model.add(MaxPooling2D(pool_size=(1,1)))
    model.add(BatchNormalization())

    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding="same"))
    model.add(MaxPooling2D(pool_size=(1,1)))
    model.add(BatchNormalization())

    model.add(Conv2D(96, kernel_size=(3, 3), activation='relu', padding="same"))
    model.add(MaxPooling2D(pool_size=(1,1)))
    model.add(BatchNormalization())

    model.add(Conv2D(96, kernel_size=(3, 3), activation='relu', padding="same"))
    model.add(MaxPooling2D(pool_size=(1,1)))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))

    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding="same"))
    model.add(MaxPooling2D(pool_size=(1,1)))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))

    model.add(Flatten())
    model.add(Dense(256, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.3))
    model.add(Dense(ytrain.shape[1], activation='softmax')) ## No of classes

    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    print('Model prepared...')
    model.fit(xtrain, ytrain, batch_size=32, epochs=epochs, validation_split=0.2)
    Y_pred = model.predict(xtest)
    Y_pred = np.argmax(Y_pred, axis=1)
    Y_true = np.argmax(ytest, axis=1)
    return Y_pred,Y_true

from sklearn.metrics import confusion_matrix

def main_est_perf_metrics(preds, y_test):

    confusion_mtx = confusion_matrix(y_test, preds)
    #cm =sum(mcm)
    """Total Counts of Confusion Matrix"""
    total = sum(sum(confusion_mtx))
    # """True Positive"""
    TP = confusion_mtx[0, 0]
    """False Positive"""
    FP = confusion_mtx[0, 1]
    """False Negative"""
    FN = confusion_mtx[1, 0]
    """True Negative"""
    TN = confusion_mtx[1, 1]
    """Accuracy Formula"""
    acc = (TP + TN) / total
    """Sensitivity Formula"""
    sen = TP / (FN + TP)
    """Specificity Formula"""
    spe = TN / (FP + TN)
    """Precision Formula"""
    pre = TP / (TP + FP)
    """Recall Formula"""
    rec = TP / (FN + TP)
    """F1 Score Formula"""
    f1_score = (2 * pre * rec) / (pre + rec)
    '''Critical Success Index '''
    CSI = TP/(TP+FN+FP)
    '''False Positivie Rate'''
    FPR = FP / (FP + TN)  # 1 - Specificity
    '''False Negative Rate'''
    FNR = FN / (TP + FN)  # 1 - Sensitivity
    '''Matthews Correlation Coefficient'''
    MCC = (TP * TN - FP * FN) / np.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))
    '''Negative Predictive Value'''
    NPV = TN / (TN + FN)  # negative predictive value
    '''Positive Predictive Value'''
    PPV = TP / (TP + FP)  # Positive Predictive Value

    # print("Accuracy:", ACC)
    # print("Sensitivity:", SEN)
    # print("Specificity:", SPE)
    # print("Precision:", PRE1)
    # print("Recall:", REC1)
    # print("F1 Score:", FSC1)
    # print("CSI:", CSI1)
    # print("FPR:", FPR1)
    # print("FNR:", FNR1)
    # print("MCC:", MCC1)
    # print("PPV:", PPV1)
    # print("NPV:", NPV1)

    return [acc, sen, spe, pre,rec,f1_score,CSI,FPR,FNR,MCC,PPV,NPV]

def oversample(X_train, y_train):
    ros = RandomOverSampler(random_state=42)
    X_resampled, y_resampled = ros.fit_resample(X_train, y_train)
    return X_resampled, y_resampled


def Tp_Analysis():
    features = np.load("Final Featres/final_features.npy")
    labels = np.load("Final Featres/Labels.npy")

    le = LabelEncoder()

    labels = le.fit_transform(labels)

    features, labels = oversample(features, labels)

    epochs = [2, 4, 6, 8, 10]  # No. of Iterations

    tr = [0.4, 0.5, 0.6, 0.7, 0.8]  # Variation of Training Percentage - takes datasets rom smaller percentage to higher percentage to give the training percentage

    options = [0, 1, 2, 3]
    COM_A = []
    COM_B = []
    COM_C = []
    COM_D = []
    COM_E = []
    COM_F = []
    COM_G = []
    COM_H = []
    COM_I = []

    for p in range(len(tr)):
        print(  '\033[46m' + '\033[30m' + "Training Percentage and Testing Percentage : " + str(tr[p] * 100) + " and " + str(
                100 - (tr[p] * 100)) + '\x1b[0m')

        xtrain, xtest, ytrain, ytest = split_data(features, labels, train_size=tr[p])

        y1train = to_categorical(ytrain)
        y1test = to_categorical(ytest)

        print('\033[46m' + '\033[30m' + "------------------------------MODEL TRAINING SECTION---------------------------------------"  + '\x1b[0m')
        Y_pred1, Y_true1 = LSTM_Model(xtrain,y1train,xtest,y1test)
        Y_pred2, Y_true2 = Bi_LSTM_MODEL(xtrain,y1train,xtest,y1test)
        Y_pred3, Y_true3 = CNN_Based_On_ResNet(xtrain,y1train,xtest,y1test)
        # Y_pred4, Y_true4 = RNN(xtrain,y1train,xtest,y1test)
        Y_pred5, Y_true5 = Hybrid_Attention_Based_Explainable_CNN(xtrain,y1train,xtest,y1test,epochs[0])
        Y_pred6, Y_true6 = Hybrid_Attention_Based_Explainable_CNN(xtrain, y1train, xtest, y1test, epochs[1])
        Y_pred7, Y_true7 = Hybrid_Attention_Based_Explainable_CNN(xtrain, y1train, xtest, y1test, epochs[2])
        Y_pred8, Y_true8 = Hybrid_Attention_Based_Explainable_CNN(xtrain, y1train, xtest, y1test, epochs[3])
        Y_pred9, Y_true9 = Hybrid_Attention_Based_Explainable_CNN(xtrain, y1train, xtest, y1test, epochs[4])



        print('\033[46m' + '\033[30m' + "________________________Metrics Evaluated from Confusion Matrix__________________________________" + '\x1b[0m')

        [ACC1, SEN1, SPE1, PRE1, REC1, FSC1, CSI1, FPR1, FNR1, MCC1, PPV1, NPV1] = main_est_perf_metrics(Y_pred1,Y_true1)
        [ACC2, SEN2, SPE2, PRE2, REC2, FSC2, CSI2, FPR2, FNR2, MCC2, PPV2, NPV2] = main_est_perf_metrics(Y_pred2, Y_true2)
        [ACC3, SEN3, SPE3, PRE3, REC3, FSC3, CSI3, FPR3, FNR3, MCC3, PPV3, NPV3] = main_est_perf_metrics(Y_pred3, Y_true3)
        # [ACC4, SEN4, SPE4, PRE4, REC4, FSC4, CSI4, FPR4, FNR4, MCC4, PPV4, NPV4] = main_est_perf_metrics(Y_pred4, Y_true4 )
        [ACC5, SEN5, SPE5, PRE5, REC5, FSC5, CSI5, FPR5, FNR5, MCC5, PPV5, NPV5] = main_est_perf_metrics(Y_pred5, Y_true5)
        [ACC6, SEN6, SPE6, PRE6, REC6, FSC6, CSI6, FPR6, FNR6, MCC6, PPV6, NPV6] = main_est_perf_metrics(Y_pred6, Y_true6)
        [ACC7, SEN7, SPE7, PRE7, REC7, FSC7, CSI7, FPR7, FNR7, MCC7, PPV7, NPV7] = main_est_perf_metrics(Y_pred7, Y_true7)
        [ACC8, SEN8, SPE8, PRE8, REC8, FSC8, CSI8, FPR8, FNR8, MCC8, PPV8, NPV8] = main_est_perf_metrics(Y_pred8, Y_true8)
        [ACC9, SEN9, SPE9, PRE9, REC9, FSC9, CSI9, FPR9, FNR9, MCC9, PPV9, NPV9] = main_est_perf_metrics(Y_pred9, Y_true9)

        print('\033[46m' + '\033[30m' +"________________________Save Metrics__________________________________" + '\x1b[0m')

        COM_A.append([ACC1, SEN1, SPE1, PRE1, REC1, FSC1, CSI1, FPR1, FNR1, MCC1, PPV1, NPV1])
        COM_B.append([ACC2, SEN2, SPE2, PRE2, REC2, FSC2, CSI2, FPR2, FNR2, MCC2, PPV2, NPV2])
        COM_C.append([ACC3, SEN3, SPE3, PRE3, REC3, FSC3, CSI3, FPR3, FNR3, MCC3, PPV3, NPV3])
        # COM_D.append([ACC4, SEN4, SPE4, PRE4, REC4, FSC4, CSI4, FPR4, FNR4, MCC4, PPV4, NPV4])
        COM_E.append([ACC5, SEN5, SPE5, PRE5, REC5, FSC5, CSI5, FPR5, FNR5, MCC5, PPV5, NPV5])
        COM_F.append([ACC6, SEN6, SPE6, PRE6, REC6, FSC6, CSI6, FPR6, FNR6, MCC6, PPV6, NPV6])
        COM_G.append([ACC7, SEN7, SPE7, PRE7, REC7, FSC7, CSI7, FPR7, FNR7, MCC7, PPV7, NPV7])
        COM_H.append([ACC8, SEN8, SPE8, PRE8, REC8, FSC8, CSI8, FPR8, FNR8, MCC8, PPV8, NPV8])
        COM_I.append([ACC9, SEN9, SPE9, PRE9, REC9, FSC9, CSI9, FPR9, FNR9, MCC9, PPV9, NPV9])


    np.save('NPY\\COM_A.npy'.format(os.getcwd()), COM_A)
    np.save('NPY\\COM_B.npy'.format(os.getcwd()), COM_B)
    np.save('NPY\\COM_C.npy'.format(os.getcwd()), COM_C)
    # np.save('NPY\\COM_D.npy'.format(os.getcwd()), COM_D)
    np.save('NPY\\COM_E.npy'.format(os.getcwd()), COM_E)
    np.save('NPY\\COM_F.npy'.format(os.getcwd()), COM_F)
    np.save('NPY\\COM_G.npy'.format(os.getcwd()), COM_G)
    np.save('NPY\\COM_H.npy'.format(os.getcwd()), COM_H)
    np.save('NPY\\COM_I.npy'.format(os.getcwd()), COM_I)

Tp_Analysis()






