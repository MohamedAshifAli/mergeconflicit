from sklearn.metrics import confusion_matrix
import keras
import pandas as pd
from imblearn.over_sampling import RandomOverSampler
from keras import Model, Sequential
from keras.applications import ResNet101
from keras.layers import BatchNormalization, Dropout, Flatten, Dense, Activation, MaxPooling2D, Conv2D, LSTM, \
    GlobalAveragePooling2D, MaxPool2D, multiply, Add, GlobalMaxPooling2D, Reshape,Lambda
from keras.utils import to_categorical
from skimage import color
from skimage.util import img_as_ubyte

from numpy.random import seed
import ast
from glob import glob
import cv2
from PIL import Image
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder
from keras.models import Sequential
import keras
import os
from keras.layers import  Embedding, LSTM,Bidirectional,SimpleRNN,Conv1D
from keras import metrics, regularizers
from keras.optimizers import SGD,RMSprop, Adam, Adadelta, Adagrad ,Adamax, Nadam
import seaborn as sns
from keras.utils import np_utils
from keras import layers
from keras import backend as K


from mealpy.swarm_based.COA import BaseCOA as  COA
from mealpy.swarm_based.GWO import  BaseGWO as GWO
from mealpy.Prop import BaseProp as Prop
from skimage.feature import graycomatrix, graycoprops
from sklearn.metrics import multilabel_confusion_matrix, accuracy_score
from termcolor import colored
seed(1)
import tensorflow
tensorflow.random.set_seed(2)
import tensorflow as tf
from keras.layers import BatchNormalization
from glob import glob
import cv2
import matplotlib.pyplot as plt
import tensorflow.compat.v1 as tf
import warnings
warnings.filterwarnings('ignore',category= UserWarning)
from keras.applications import ResNet101
from keras.models import Model
from keras.layers import Dropout, Flatten, BatchNormalization, Dense, Activation
from pytorch_grad_cam import (GradCAMPlusPlus, FullGrad)
from pytorch_grad_cam import GuidedBackpropReLUModel
from pytorch_grad_cam.utils.image import (show_cam_on_image, deprocess_image, preprocess_image)
from torchvision import models
resnet50_model = models.resnet50(pretrained=True)


def Guided_Propagation_Model(rgb_img, CAM_Model=None):
    print("Guided_Propagation_Model")
    target_layers = [resnet50_model.layer4] ## Base Model
    rgb_img = rgb_img[:, :, ::-1]
    rgb_img = np.float32(rgb_img) / 255
    input_tensor = preprocess_image(rgb_img,
                                    mean=[0.485, 0.456, 0.406],
                                    std=[0.229, 0.224, 0.225])
    targets = None
    cam_algorithm = CAM_Model ## CAM Algorithm
    with cam_algorithm(model=resnet50_model,
                       target_layers=target_layers
                       ) as cam:
        cam.batch_size = 32
        grayscale_cam = cam(input_tensor=input_tensor,
                            targets=targets,
                            aug_smooth=False,
                            eigen_smooth=False)

        grayscale_cam = grayscale_cam[0, :]
        # , use_cuda = False
        cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)
        cam_image = cv2.cvtColor(cam_image, cv2.COLOR_RGB2BGR)
    gb_model = GuidedBackpropReLUModel(model=resnet50_model, use_cuda=False)
    gb = gb_model(input_tensor, target_category=None)
    cam_mask = cv2.merge([grayscale_cam, grayscale_cam, grayscale_cam]) ## Merge all Outputs
    cam_gb = deprocess_image(cam_mask * gb)
    return cam_gb

def f(x):
    return ast.literal_eval(x.rstrip('\r\n')) #convrt the annotations strings into their original data types integr for the usage in nprogram


def getBounds(geometry):#gettg the calculated bounding box coordinates xmin, ymin, xmax, ymax.

    try:
        arr = np.array(geometry).T
        xmin = np.min(arr[0])
        ymin = np.min(arr[1])
        xmax = np.max(arr[0])
        ymax = np.max(arr[1])
        return (xmin, ymin, xmax, ymax)
    except:
        return np.nan


def getWidth(bounds): #gettg widh
    try:
        (xmin, ymin, xmax, ymax) = bounds
        return np.abs(xmax - xmin)
    except:
        return np.nan


def getHeight(bounds):#gettg height points from annottaions
    try:
        (xmin, ymin, xmax, ymax) = bounds
        return np.abs(ymax - ymin)
    except:
        return np.nan


def upconv_concat(bottom_a, bottom_b, n_filter, k_size, stride, padding='VALID'):
    up_conv = tf.layers.conv2d_transpose(bottom_a, filters=n_filter, kernel_size=[k_size, k_size],
                                         strides=stride, padding=padding)
    return tf.concat([up_conv, bottom_b], axis=-1)


def conv_layer(bottom, k_size, num_outputs, stride, padding='SAME'):
    input_channels = int(bottom.get_shape()[-1])
    weights = tf.Variable(tf.truncated_normal(shape=[k_size, k_size, input_channels, num_outputs], dtype=tf.float32,
                                              stddev=np.sqrt(1.0 / (k_size * k_size * input_channels))))
    biases = tf.Variable(tf.constant(0, dtype=tf.float32, shape=[num_outputs]))
    conv = tf.nn.conv2d(bottom, weights, strides=[1, stride, stride, 1], padding=padding)
    bias = tf.reshape(tf.nn.bias_add(conv, biases), tf.shape(conv))
    relu = tf.nn.relu(bias)

    return relu


def calc_loss(num_outputs, encoder_output, one_dim_kernel, sigma_pixel, spatial_kernel, batch_size):#lox calucltion
    num_sum = tf.constant(0.0, dtype=tf.float32)
    for depth in range(num_outputs):
        softmax_layer = encoder_output[:, :, :,
                        depth:depth + 1]  # take each channel of the output image from encoder_model
        extracted_pixels = tf.nn.conv2d(softmax_layer, one_dim_kernel, strides=[1, 1, 1, 1], padding='SAME')

        intensity_sq_dif = tf.squared_difference(extracted_pixels, softmax_layer)
        intensity_values = tf.exp(tf.divide(tf.negative(intensity_sq_dif), sigma_pixel))

        weights = tf.multiply(intensity_values, spatial_kernel)
        # Reshape Input Softmax Layer for correct dimensions
        u_pixels = tf.reshape(softmax_layer, [batch_size, 224, 224])

        # Calculate entire numerator
        numerator_inner_sum = tf.reduce_sum(tf.multiply(weights, extracted_pixels), axis=3)
        numerator_outer_sum = tf.multiply(u_pixels, numerator_inner_sum)
        numerator = tf.reduce_sum(numerator_outer_sum)

        # Calculate denominator
        denominator_inner_sum = tf.reduce_sum(weights, axis=3)
        denominator_outer_sum = tf.multiply(u_pixels, denominator_inner_sum)
        denominator = tf.reduce_sum(denominator_outer_sum)

        processed_value = numerator / denominator
        num_sum += processed_value

    return num_outputs - num_sum

def Hybrid_Attention_Module_Segmentation(x):
    # Get Weights from previous layers
    all_x = x.node.layer.trainable_weights
    ## From all weights select zero th layer weights
    sel_x = all_x[0]
    w = np.array(sel_x)
    x = tf.convert_to_tensor(x)
    ## fed selected weights into Triplet attention Module
    weights = HybridAttention(x).forward()
    # Reshape according to old weight
    weights = weights.reshape(w.shape[0], w.shape[1],w.shape[2],w.shape[3])
    ## Convert it into tensor
    weights = tf.convert_to_tensor(weights)
    ## replace old weight by Triplet module generated weights
    all_x[0] = weights
    ## updated all weights in x
    x.node.layer.trainable_weights_ = all_x
    return x
def Hybrid_WNet_train(X_train):
    tf.disable_v2_behavior()
    # tf.compat.v1.disable_eager_execution()
    X_train = X_train.astype('float32') / 255.  # Standardizing the data
    X_train = np.reshape(X_train, (len(X_train), 224, 224, 1))  # reshape it to (1140, 224, 224, 1) for feeding to model

    height = width = 224
    channels = 1
    keep_prob = tf.placeholder_with_default(1.0, shape=())
    num_outputs = 3  # background, cell.
    num_epochs = 1500
    batch_size = 0
    r = 5
    sigma_dist = 4
    sigma_pixel = tf.square(tf.constant(10.0))
    input_img = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, height, width, channels], name='input_tensor')
    variance_epsilon = 0.0001
    with tf.name_scope('Encoder'):

        conv_1_1 = tf.layers.conv2d(input_img, 64, 3, padding='same', activation='relu')
        conv_1_1_with_attention = Hybrid_Attention_Module_Segmentation(conv_1_1)
        mean, variance = tf.nn.moments(conv_1_1_with_attention, [0, 1, 2])
        conv_1_1_bn = tf.nn.batch_normalization(conv_1_1, mean, variance, None, None, variance_epsilon)

        conv_1_1_drop = tf.layers.dropout(conv_1_1_bn, keep_prob)
        conv_1_2 = tf.layers.conv2d(conv_1_1_drop, 64, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_1_2, [0, 1, 2])
        conv_1_2_bn = tf.nn.batch_normalization(conv_1_2, mean, variance, None, None, variance_epsilon)
        conv_1_2_drop = tf.layers.dropout(conv_1_2_bn, keep_prob)

        pool_1 = tf.layers.max_pooling2d(conv_1_2_drop, pool_size=2, strides=2, padding="valid")

        conv_2_1 = tf.layers.separable_conv2d(pool_1, 128, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_2_1, [0, 1, 2])
        conv_2_1_bn = tf.nn.batch_normalization(conv_2_1, mean, variance, None, None, variance_epsilon)
        conv_2_1_drop = tf.layers.dropout(conv_2_1_bn, keep_prob)

        conv_2_2 = tf.layers.separable_conv2d(conv_2_1_drop, 128, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_2_2, [0, 1, 2])
        conv_2_2_bn = tf.nn.batch_normalization(conv_2_2, mean, variance, None, None, variance_epsilon)
        conv_2_2_drop = tf.layers.dropout(conv_2_2_bn, keep_prob)

        pool_2 = tf.layers.max_pooling2d(conv_2_2_drop, pool_size=2, strides=2, padding="valid")

        conv_3_1 = tf.layers.separable_conv2d(pool_2, 256, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_3_1, [0, 1, 2])
        conv_3_1_bn = tf.nn.batch_normalization(conv_3_1, mean, variance, None, None, variance_epsilon)
        conv_3_1_drop = tf.layers.dropout(conv_3_1_bn, keep_prob)

        conv_3_2 = tf.layers.separable_conv2d(conv_3_1_drop, 256, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_3_2, [0, 1, 2])
        conv_3_2_bn = tf.nn.batch_normalization(conv_3_2, mean, variance, None, None, variance_epsilon)
        conv_3_2_drop = tf.layers.dropout(conv_3_2_bn, keep_prob)

        pool_3 = tf.layers.max_pooling2d(conv_3_2_drop, pool_size=2, strides=2, padding="valid")

        conv_4_1 = tf.layers.separable_conv2d(pool_3, 512, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_4_1, [0, 1, 2])
        conv_4_1_bn = tf.nn.batch_normalization(conv_4_1, mean, variance, None, None, variance_epsilon)
        conv_4_1_drop = tf.layers.dropout(conv_4_1_bn, keep_prob)

        conv_4_2 = tf.layers.separable_conv2d(conv_4_1_drop, 512, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_4_2, [0, 1, 2])
        conv_4_2_bn = tf.nn.batch_normalization(conv_4_2, mean, variance, None, None, variance_epsilon)
        conv_4_2_drop = tf.layers.dropout(conv_4_2_bn, keep_prob)

        pool_4 = tf.layers.max_pooling2d(conv_4_2_drop, pool_size=2, strides=2, padding="valid")

        conv_5_1 = tf.layers.separable_conv2d(pool_4, 1024, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_5_1, [0, 1, 2])
        conv_5_1_bn = tf.nn.batch_normalization(conv_5_1, mean, variance, None, None, variance_epsilon)
        conv_5_1_drop = tf.layers.dropout(conv_5_1_bn, keep_prob)

        conv_5_2 = tf.layers.separable_conv2d(conv_5_1_drop, 1024, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_5_2, [0, 1, 2])
        conv_5_2_bn = tf.nn.batch_normalization(conv_5_2, mean, variance, None, None, variance_epsilon)
        conv_5_2_drop = tf.layers.dropout(conv_5_2_bn, keep_prob)
        upconv_1 = upconv_concat(conv_5_2_drop, conv_4_2_drop, n_filter=512, k_size=2, stride=2)

        conv_6_1 = tf.layers.separable_conv2d(upconv_1, 512, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_6_1, [0, 1, 2])
        conv_6_1_bn = tf.nn.batch_normalization(conv_6_1, mean, variance, None, None, variance_epsilon)
        conv_6_1_drop = tf.layers.dropout(conv_6_1_bn, keep_prob)

        conv_6_2 = tf.layers.separable_conv2d(conv_6_1_drop, 512, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_6_2, [0, 1, 2])
        conv_6_2_bn = tf.nn.batch_normalization(conv_6_2, mean, variance, None, None, variance_epsilon)
        conv_6_2_drop = tf.layers.dropout(conv_6_2_bn, keep_prob)

        upconv_2 = upconv_concat(conv_6_2_drop, conv_3_2_drop, n_filter=256, k_size=2, stride=2)

        conv_7_1 = tf.layers.separable_conv2d(upconv_2, 256, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_7_1, [0, 1, 2])
        conv_7_1_bn = tf.nn.batch_normalization(conv_7_1, mean, variance, None, None, variance_epsilon)
        conv_7_1_drop = tf.layers.dropout(conv_7_1_bn, keep_prob)

        conv_7_2 = tf.layers.separable_conv2d(conv_7_1_drop, 256, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_7_2, [0, 1, 2])
        conv_7_2_bn = tf.nn.batch_normalization(conv_7_2, mean, variance, None, None, variance_epsilon)
        conv_7_2_drop = tf.layers.dropout(conv_7_2_bn, keep_prob)

        upconv_3 = upconv_concat(conv_7_2_drop, conv_2_2_drop, n_filter=128, k_size=2, stride=2)

        conv_8_1 = tf.layers.separable_conv2d(upconv_3, 128, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_8_1, [0, 1, 2])
        conv_8_1_bn = tf.nn.batch_normalization(conv_8_1, mean, variance, None, None, variance_epsilon)
        conv_8_1_drop = tf.layers.dropout(conv_8_1_bn, keep_prob)

        conv_8_2 = tf.layers.separable_conv2d(conv_8_1_drop, 128, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_8_2, [0, 1, 2])
        conv_8_2_bn = tf.nn.batch_normalization(conv_8_2, mean, variance, None, None, variance_epsilon)
        conv_8_2_drop = tf.layers.dropout(conv_8_2_bn, keep_prob)

        upconv_4 = upconv_concat(conv_8_2_drop, conv_1_2_drop, n_filter=64, k_size=2, stride=2)

        conv_9_1 = tf.layers.separable_conv2d(upconv_4, 64, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_9_1, [0, 1, 2])
        conv_9_1_bn = tf.nn.batch_normalization(conv_9_1, mean, variance, None, None, variance_epsilon)
        conv_9_1_drop = tf.layers.dropout(conv_9_1_bn, keep_prob)

        conv_9_2 = tf.layers.separable_conv2d(conv_9_1_drop, 64, 3, padding='same', activation='relu')
        mean, variance = tf.nn.moments(conv_9_2, [0, 1, 2])
        conv_9_2_bn = tf.nn.batch_normalization(conv_9_2, mean, variance, None, None, variance_epsilon)
        conv_9_2_drop = tf.layers.dropout(conv_9_2_bn, keep_prob)

        conv = tf.layers.conv2d(conv_9_2_drop, num_outputs, kernel_size=1, strides=1, padding='same', activation='relu')
        print(conv.shape, "conv")
        encoder_output = tf.nn.softmax(conv, axis=3, name="output_tensor")

        init = tf.global_variables_initializer()

        s = 2 * r + 1
        spatial_kernel = np.zeros((s, s), dtype=np.float32)
        for y in range(s):
            for x in range(s):
                # calculate squared euclidean distance
                dist = (x - r) * (x - r) + (y - r) * (y - r)
                if dist < (r * r):
                    spatial_kernel[y][x] = np.exp((-dist) / sigma_dist)

        spatial_kernel = tf.constant(spatial_kernel.reshape(-1), dtype=tf.float32)

        # create one dimensional kernel

        s = 2 * r + 1
        one_dim_kernel = np.zeros((s, s, (s * s)))
        for i in range(s * s):
            one_dim_kernel[int(i / s)][i % s][i] = 1.0
        one_dim_kernel = one_dim_kernel.reshape(s, s, 1, (s * s))
        one_dim_kernel = tf.constant(one_dim_kernel, dtype=tf.float32)

        loss = calc_loss(num_outputs, encoder_output, one_dim_kernel, sigma_pixel, spatial_kernel, batch_size)
        soft_cut_norm_loss = tf.reduce_mean(loss)
        with tf.name_scope("optimizer"):
            norm_cut_opt = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(soft_cut_norm_loss)  # optimizer

        init = tf.global_variables_initializer()

        with tf.Session() as sess:
            sess.run(init)
        for epoch in range(num_epochs):
            print("epoch:", epoch)
        count = 0
        batch_start_index = 0
        while (count != 1):
            X_train_batch = X_train[batch_start_index: batch_start_index + batch_size]
            _, train_loss = sess.run([norm_cut_opt, soft_cut_norm_loss],
                                     feed_dict={input_img: X_train_batch, keep_prob: 0.7})
            batch_start_index += batch_size
            count += 1
            print("Train loss after epoch ", str(epoch), "is", str(train_loss))
        #     saved_path = saver.save(sess, './my-model')
        output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(),
                                                                        ["Encoder/output_tensor"])
        with tf.gfile.GFile("wnetmodel.pb", "wb") as f:
            f.write(output_graph_def.SerializeToString())

    with tf.Session() as sess:
        sess.run(init)
        #     saver.restore(sess, './my-model')
        output = sess.run(encoder_output, feed_dict={input_img: X_train})
    return output


def sharpen(img):

    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])
    sharpened_image = cv2.filter2D(img, -1, kernel)
    return sharpened_image

def preprocessing_object_detection():
    Features = []
    label = []
    df = pd.read_csv("Datasets/annotations.csv",
                     converters={'geometry': f})
    # Create bounds, width and height
    df.loc[:, 'bounds'] = df.loc[:, 'geometry'].apply(getBounds)
    df.loc[:, 'width'] = df.loc[:, 'bounds'].apply(getWidth)
    df.loc[:, 'height'] = df.loc[:, 'bounds'].apply(getHeight)

    image_files = glob('Datasets/ALlimages/*.*')
    unique, counts = np.unique(df['class'], return_counts=True)
    cnt = 1
    for image in image_files:
        filename = os.path.basename(image)
        all_indexes = np.where(df['image_id'] == filename)[0]
        img = cv2.imread(image)
        ### Image Enhancement
        image = img
        for ind in all_indexes:
            print("Preprocessing : "+str(cnt))
            img=image.copy()
            label.append(df['class'][ind])
            coordinates = df['bounds'][ind]
            xB = coordinates[2]
            xA = coordinates[0]
            yB = coordinates[3]
            yA = coordinates[1]
            cv2.rectangle(img, (xA, yA), (xB, yB), (0, 0, 0), 0)
            img = img[yA:yB, xA:xB]
            getSharpen = sharpen(img)
            dst = cv2.fastNlMeansDenoisingColored(getSharpen, None, 5, 5, 7, 21)
            resized = cv2.resize(dst, (224, 224), interpolation=cv2.INTER_AREA)
            filename = 'Detected Objects\\img__' + str(cnt) + '.jpg'
            cv2.imwrite(filename, resized)
            cnt += 1
            # Resize image to (224, 224)
            Features.append(resized)  # append to list
    feat = np.asarray(Features)  # convert to numpy array
    lab = np.asarray(label)
    np.save("Features/Images.npy",feat)
    np.save("Features/Labels.npy",lab)


def Segmentation():
    images = np.load("Features/Images.npy")
    # images = images[:, :, :, 0]
    all_outputs = []
    cntt=0
    batch_size = 50
    for i in range(0, len(images), batch_size):
        print("\033[93mSegmentation\033[0m : " + str(cntt))
        end_index = min(i + batch_size, len(images))
        output =Hybrid_WNet_train(images[i:end_index])
        cntt +=1
        for k in output:
            all_outputs.append(k)
            plt.imshow(k)
    all_outputss = np.asarray(all_outputs)
    np.save("segmentatione.npy", all_outputs)


def entropy(glcm):
    entropy1 = -np.sum(glcm * np.log2(glcm + (glcm == 0)))
    return entropy1
# //////////shape and Structural Feauture Extrcation

def ldp_process(photo):
    def assign_bit(picture, x, y, c1, c2, d):  # assign bit according to degree and neighbouring pixel
        # a and b are 1 if increasing and 0 if decreasing
        if d == 0:
            a = 0
            b = 0
            try:
                if picture[c1][c2 + 1] >= picture[c1][c2]:
                    a = 1
                if picture[x][y + 1] >= picture[x][y]:
                    b = 1
            except:
                pass
        if d == 45:
            a = 0
            b = 0
            try:
                if picture[c1 - 1][c2 + 1] >= picture[c1][c2]:
                    a = 1
                if picture[x - 1][y + 1] >= picture[x][y]:
                    b = 1
            except:
                pass
        if d == 90:
            a = 0
            b = 0
            try:
                if picture[c1 - 1][c2] >= picture[c1][c2]:
                    a = 1
                if picture[x - 1][y] >= picture[x][y]:
                    b = 1
            except:
                pass
        if d == 135:
            a = 0
            b = 0
            try:
                if picture[c1 - 1][c2 - 1] >= picture[c1][c2]:
                    a = 1
                if picture[x - 1][y - 1] >= picture[x][y]:
                    b = 1
            except:
                pass
        if a == b:  # if monotonically increasing or decreasing than 0
            return "0"
        else:  # if turning point
            return "1"

        return bit

    def local_der_val(picture, x, y):  # calculating local derivative pattern value of a pixel
        thirtytwo_bit_binary = []
        centre = picture[x][y]
        c1 = x
        c2 = y
        decimal_val = 0
        # starting from top left,assigning bit to pixels clockwise at 0 degree
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y - 1, c1, c2, 0))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y, c1, c2, 0))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y + 1, c1, c2, 0))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y + 1, c1, c2, 0))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y + 1, c1, c2, 0))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y, c1, c2, 0))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y - 1, c1, c2, 0))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y - 1, c1, c2, 0))

        # starting from top left,assigning bit to pixels clockwise at 45 degree
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y - 1, c1, c2, 45))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y, c1, c2, 45))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y + 1, c1, c2, 45))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y + 1, c1, c2, 45))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y + 1, c1, c2, 45))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y, c1, c2, 45))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y - 1, c1, c2, 45))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y - 1, c1, c2, 45))

        # starting from top left,assigning bit to pixels clockwise at 90 degree
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y - 1, c1, c2, 90))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y, c1, c2, 90))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y + 1, c1, c2, 90))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y + 1, c1, c2, 90))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y + 1, c1, c2, 90))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y, c1, c2, 90))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y - 1, c1, c2, 90))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y - 1, c1, c2, 90))

        # starting from top left,assigning bit to pixels clockwise at 135 degree
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y - 1, c1, c2, 135))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y, c1, c2, 135))
        thirtytwo_bit_binary.append(assign_bit(picture, x - 1, y + 1, c1, c2, 135))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y + 1, c1, c2, 135))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y + 1, c1, c2, 135))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y, c1, c2, 135))
        thirtytwo_bit_binary.append(assign_bit(picture, x + 1, y - 1, c1, c2, 135))
        thirtytwo_bit_binary.append(assign_bit(picture, x, y - 1, c1, c2, 135))

        str1 = ""
        l = str1.join(thirtytwo_bit_binary)  # 32 bit binary number
        decimal_val = int(l, 2)  # 32 bit binary to decimal number
        return decimal_val

    m, n, _ = photo.shape
    # m, n, = photo.shape
    photo = cv2.cvtColor(photo, cv2.COLOR_BGR2GRAY)  # converting image to grayscale
    ldp_photo = np.zeros((m, n))
    # converting image to ldp
    for i in range(0, m):
        for j in range(0, n):
            ldp_photo[i, j] = local_der_val(photo, i, j)

    return ldp_photo
#canny shape
def edge_detection(img):
    a = (img * 255).astype("uint8")
    edges = cv2.Canny(a, 100, 200)  # Add threshold2 value (e.g., 200)
    return edges
def shape_structure_feautures(img):
    canny = edge_detection(img)
    ldp = ldp_process(img)
    combined_image = ldp + canny

    return combined_image

def main_resnet_feat_ext(base_model):
    x = base_model.output
    x = Dropout(0.05)(x)
    x = Flatten()(x)
    x = BatchNormalization()(x)
    x = Dense(1024, kernel_initializer='he_uniform')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dropout(0.05)(x)
    x = Dense(1024, kernel_initializer='he_uniform')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dropout(0.05)(x)
    x = Dense(1024, kernel_initializer='he_uniform')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dropout(0.05)(x)
    predictions = Dense(100, activation='softmax')(x)
    model_feat = Model(inputs=base_model.input, outputs=predictions)
    return model_feat
def feat_resnet(array, input_shape):
    base_model = ResNet101(include_top=False, weights='imagenet', input_shape=input_shape)
    Model_feat = main_resnet_feat_ext(base_model)
    feat = Model_feat.predict(array)
    return feat
def Resnet101_Feature_Extraction(image1):
    """Residual Network (ResNet) is a deep learning model used for computer vision applications.
    It is a Convolutional Neural Network (CNN) architecture designed to support hundreds or thousands of convolutional layers
    Every layer of a ResNet is composed of several blocks. This is because when ResNets go deeper,
    they normally do it by increasing the number of operations within a block,
    but the number of total layers remains the same.ResNet-101 is a convolutional neural network that is 101 layers deep.
    You can load a pretrained version of the network trained on more than a million images from the ImageNet database.
    he pretrained network can classify images into 1000 object categories"""
    input_shape = (image1.shape[1], image1.shape[2], image1.shape[3])
    resnet_feat = feat_resnet(image1,input_shape)  # resnet feature extraction
    return resnet_feat

#LTP PAttern
def get_pixel_of_mltp_left(image, center, x, y):
    new_value = 0
    try:
        if image[x][y] < center:
            new_value = 1 #-1
        elif image[x][y] == center:
            new_value = 0 # 0
        elif image[x][y] > center:
            new_value = 0  #1
    except:
        pass
    return new_value

def get_pixel_of_mltp_right(image, center, x, y):
    new_value = 0
    try:
        if image[x][y] < center:
            new_value = 0  #-1
        elif image[x][y] == center:
            new_value = 0 # 0
        elif image[x][y] > center:
            new_value = 1  #1
    except:
        pass
    return new_value

def calculated_pixel_of_mltp_right(image, x, y):
    val_ar = []
    center = image[x][y]
    # top_left
    val_ar.append(get_pixel_of_mltp_right(image, center, x - 1, y - 1))
    # top
    val_ar.append(get_pixel_of_mltp_right(image, center, x - 1, y))
    # top_right
    val_ar.append(get_pixel_of_mltp_right(image, center, x - 1, y + 1))
    # right
    val_ar.append(get_pixel_of_mltp_right(image, center, x, y + 1))
    # bottom_right
    val_ar.append(get_pixel_of_mltp_right(image, center, x + 1, y + 1))
    # bottom
    val_ar.append(get_pixel_of_mltp_right(image, center, x + 1, y))
    # bottom_left
    val_ar.append(get_pixel_of_mltp_right(image, center, x + 1, y - 1))
    # left
    val_ar.append(get_pixel_of_mltp_right(image, center, x, y - 1))
    # Now, we need to convert binary values to decimal
    power_val = [1, 2, 4, 8, 16, 32, 64, 128]
    val = 0
    for i in range(len(val_ar)):
        val += val_ar[i] * power_val[i]
    return val

def calculated_pixel_of_mltp_left(image, x, y):
    val_ar = []
    center = image[x][y]
    # top_left
    val_ar.append(get_pixel_of_mltp_left(image, center, x - 1, y - 1))
    # top
    val_ar.append(get_pixel_of_mltp_left(image, center, x - 1, y))
    # top_right
    val_ar.append(get_pixel_of_mltp_left(image, center, x - 1, y + 1))
    # right
    val_ar.append(get_pixel_of_mltp_left(image, center, x, y + 1))
    # bottom_right
    val_ar.append(get_pixel_of_mltp_left(image, center, x + 1, y + 1))
    # bottom
    val_ar.append(get_pixel_of_mltp_left(image, center, x + 1, y))
    # bottom_left
    val_ar.append(get_pixel_of_mltp_left(image, center, x + 1, y - 1))
    # left
    val_ar.append(get_pixel_of_mltp_left(image, center, x, y - 1))
    # Now, we need to convert binary values to decimal
    power_val = [1, 2, 4, 8, 16, 32, 64, 128]
    val = 0
    for i in range(len(val_ar)):
        val += val_ar[i] * power_val[i]
    return val
#LTP PAttern
def Modified_ternary_texture(img):
    img = cv2. cvtColor(img, cv2.COLOR_BGR2GRAY)
    height, width = img.shape[0], img.shape[1]
    mltp_right = np.zeros((height, width), np.uint8)
    mltp_left = np.zeros((height, width), np.uint8)
    for i in range(0, height):
        for j in range(0, width):
            mltp_right[i, j] = calculated_pixel_of_mltp_right(img, i, j)
    for i in range(0, height):
        for j in range(0, width):
            mltp_left[i, j] = calculated_pixel_of_mltp_left(img, i, j)
    img_ltp = cv2.addWeighted(mltp_right, 0.5, mltp_left, 0.5, 0)
    return img_ltp

#GLCM Features

def GLCM_Feature_Extraction(img):
    rgbImg = img
    grayImg = img_as_ubyte(color.rgb2gray(rgbImg))
    distances = [1, 2, 3]
    angles = [0, np.pi / 4, np.pi / 2, 3 * np.pi / 4]
    glcm = graycomatrix(grayImg, distances=distances, angles=angles, symmetric=True, normed=True)
    dis = (graycoprops(glcm, 'dissimilarity'))
    dis = np.mean(dis)
    eng = (graycoprops(glcm, 'energy'))
    eng = np.mean(eng)
    hom = (graycoprops(glcm, 'homogeneity'))
    hom = np.mean(hom)
    cor = (graycoprops(glcm, 'correlation'))
    cor = np.mean(cor)
    con = (graycoprops(glcm, 'contrast'))
    con = np.mean(con)
    en = entropy(glcm)
    feat = np.hstack((dis, eng, hom, cor, con, en))
    return feat

def Feature_Extraction():
    images = np.load("segmentatione.npy")
    features = []
    reimg = []

    cnt = 0
    for i in range(len(images)):
        print(colored("Feature Extraction :  >> "+ str(cnt), color='blue'))
        img = cv2.resize(images[i], (32, 32))
        im1 = Modified_ternary_texture(img)
        im3 = GLCM_Feature_Extraction(img)
        im4 = shape_structure_feautures(img)
        im4 = im4.reshape(1, im4.shape[0] * im4.shape[1])
        im1 = im1.reshape(1, im1.shape[0] * im1.shape[1])
        im3 = im3.reshape(1, im3.shape[0])
        im = np.hstack((im1,im3, im4))  # Concatenate all features
        im4 = img.reshape(1,img.shape[0],img.shape[1],img.shape[2])
        features.append(im)
        reimg.append(im4)
        cnt+=1

    feat1 = np.vstack(features)
    arra = np.vstack(reimg)
    feat2 = Resnet101_Feature_Extraction(arra)
    final_features = np.hstack((feat1, feat2))
    np.save("Features/final_features.npy", final_features)

def resize_features(features):
    new_height = 224
    new_width = 224
    resized_features = []
    for feature in features:
        resized_image = cv2.resize(feature, (new_width, new_height))
        var = np.zeros((new_height, new_width, 3))
        var[:, :, 0] = resized_image
        resized_features.append(var)
    return np.array(resized_features)

def split_data(features, labels,  train_size):
    total_data = len(features)
    indices = np.arange(total_data)
    np.random.shuffle(indices)

    split_index = int(total_data * (1 - train_size))
    train_indices = indices[:split_index]
    test_indices = indices[split_index:]

    xtrain = features[train_indices]
    xtest = features[test_indices]
    ytrain = labels[train_indices]
    ytest = labels[test_indices]

    return xtrain, xtest, ytrain, ytest


def LSTM_Model(xtrain, ytrain, xtest, ytest,epochs):
    print(
        '\033[46m' + '\033[30m' + "________________________LSTM Model prepared...__________________________________" + '\x1b[0m')

    xtrain = xtrain.reshape(xtrain.shape[0], xtrain.shape[1] // 2, 2)
    xtest = xtest.reshape(xtest.shape[0], xtest.shape[1] // 2, 2)

    model = Sequential()
    model.add(LSTM(32, return_sequences=False,input_shape = xtrain.shape[1:] ))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(ytrain.shape[1], activation="softmax"))

    model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=0.001, rho=0.9, epsilon=1e-6), metrics=['acc'])

    print(
        '\033[46m' + '\033[30m' + "________________________Training LSTM Model ..__________________________________" + '\x1b[0m')

    # ytrain = resize_features(ytrain)

    model.fit(xtrain, ytrain, batch_size=32, epochs=epochs, validation_split=0.2)

    Y_pred = model.predict(xtest)
    Y_pred = np.argmax(Y_pred, axis=1)
    Y_true = np.argmax(ytest, axis=1)

    return Y_pred, Y_true


def Bi_LSTM_MODEL(xtrain, ytrain, xtest, ytest,epochs):

    xtrain = xtrain.reshape(xtrain.shape[0], xtrain.shape[1] // 2, 2 )
    xtest = xtest.reshape(xtest.shape[0], xtest.shape[1] // 2, 2 )

    model = Sequential()
    model.add(Bidirectional(LSTM(128, activation='relu', return_sequences=False, input_shape = xtrain.shape[1:])))
    model.add(Dropout(0.2))
    model.add(Dense(ytrain.shape[1], activation='softmax'))

    model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=0.001, rho=0.9, epsilon=1e-6), metrics=['acc'])

    print(        '\033[46m' + '\033[30m' + "________________________Bi-LSTM Model prepared...__________________________________" + '\x1b[0m')


    model.fit(xtrain, ytrain, batch_size=32, epochs=epochs, validation_split=0.2)

    Y_pred = model.predict(xtest)
    Y_pred = np.argmax(Y_pred, axis=1)
    Y_true = np.argmax(ytest, axis=1)

    return Y_pred, Y_true

def CNN_Based_On_ResNet(xtrain, ytrain, xtest, ytest,epochs):

    xtrain = resize_features(xtrain)
    xtest = resize_features(xtest)

    base_layer = tensorflow.keras.applications.ResNet50(
        include_top=False,
        weights="imagenet",
        input_tensor=None,
        input_shape=(224, 224, 3),
        pooling=None,
    )

    model = Sequential([
        base_layer,
        layers.Flatten(),
        layers.Dense(32, activation="relu"),
        layers.BatchNormalization(),
        layers.Dense(ytrain.shape[1], activation="softmax") ])

    model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=0.001, rho=0.9, epsilon=1e-6), metrics=['acc'])

    print('\033[46m' + '\033[30m' + "________________________CNN_Based_On_ResNet...__________________________________" + '\x1b[0m')

    model.fit(xtrain, ytrain, batch_size=32, epochs=epochs, validation_split=0.2)

    Y_pred = model.predict(xtest)
    Y_pred = np.argmax(Y_pred, axis=1)
    Y_true = np.argmax(ytest, axis=1)

    return Y_pred, Y_true


def resize_data(data):
    resized_data = []
    for img in data:
        resized_img = cv2.resize(img, (64, 64))
        resized_img = resized_img.reshape(64, 64, 1)
        resized_data.append(resized_img)
    return np.array(resized_data)

#//////////////An Adaptive Attention Fusion Mechanism Convolutional Network
def Adaptive_Attention_Fusion_Mechanism_CNN(xtrain, ytrain, xtest, ytest, epochs):
    xtrain = xtrain.reshape(xtrain.shape[0], xtrain.shape[1] // 2, 2, 1)
    xtest = xtest.reshape(xtest.shape[0], xtest.shape[1] // 2, 2, 1)

    model = Sequential()
    model.add(Conv2D(filters=8, kernel_size=(5, 5), padding='Same', activation='relu', input_shape=(xtrain.shape[1:])))
    model.add(MaxPool2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))
    model.add(Flatten())
    model.add(Dense(ytrain.shape[1], activation='softmax'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    print('\033[46m' + '\033[30m' + "------------ADAPTIVE_ATTENTION_FUSION_MECHANISM_CNN...____" + '\x1b[0m')

    model.fit(xtrain, ytrain, batch_size=32, epochs=epochs, validation_split=0.2)

    Y_pred = model.predict(xtest)
    Y_pred = np.argmax(Y_pred, axis=1)
    Y_true = np.argmax(ytest, axis=1)
    return Y_pred, Y_true


def position_attention(input_feature, ratio=8):
    # channel_axis = 1 if K.image_data_format() == "channels_first" else -1
    channel_axis = -1
    channel = input_feature.shape[channel_axis]

    shared_layer_one = Dense(channel // ratio,
                             activation='relu',
                             kernel_initializer='he_normal',
                             use_bias=True,
                             bias_initializer='zeros')
    shared_layer_two = Dense(channel,
                             kernel_initializer='he_normal',
                             use_bias=True,
                             bias_initializer='zeros')

    avg_pool = GlobalAveragePooling2D()(input_feature)
    avg_pool = Reshape((1, 1, channel))(avg_pool)
    assert avg_pool.shape[1:] == (1, 1, channel)
    avg_pool = shared_layer_one(avg_pool)
    assert avg_pool.shape[1:] == (1, 1, channel // ratio)
    avg_pool = shared_layer_two(avg_pool)
    assert avg_pool.shape[1:] == (1, 1, channel)

    max_pool = GlobalMaxPooling2D()(input_feature)
    max_pool = Reshape((1, 1, channel))(max_pool)
    assert max_pool.shape[1:] == (1, 1, channel)
    max_pool = shared_layer_one(max_pool)
    assert max_pool.shape[1:] == (1, 1, channel // ratio)
    max_pool = shared_layer_two(max_pool)
    assert max_pool.shape[1:] == (1, 1, channel)

    max_pool = GlobalMaxPooling2D()(input_feature)
    max_pool = Reshape((1, 1, channel))(max_pool)
    assert max_pool.shape[1:] == (1, 1, channel)
    max_pool = shared_layer_one(max_pool)
    assert max_pool.shape[1:] == (1, 1, channel // ratio)
    max_pool = shared_layer_two(max_pool)
    assert max_pool.shape[1:] == (1, 1, channel)

    pam_feature = Add()([avg_pool, max_pool])
    pam_feature = Activation('softmax')(pam_feature)

    return multiply([input_feature, pam_feature])

class ZeroChannelAttention:
    def __init__(self):
        self.avg_pool =  GlobalAveragePooling2D()
        self.max_pool = GlobalMaxPooling2D()

        self.sigmoid =Activation('sigmoid')

    def forward(self, x):
        self.avg_pool=self.avg_pool(x)
        self.max_pool=self.max_pool(x)
        self.x=Add()([self.avg_pool, self.max_pool])
        self.x=self.sigmoid(self.x)
        return self.x

class ZeroSpatialAttention:
    def __init__(self):
        self.sigmoid =Activation('sigmoid')

    def forward(self, X):
        self.avg_out =  Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(X)
        self.max_out= Lambda(lambda x: K.max(x, axis=3, keepdims=True))(X)
        self.x = Add()([self.avg_out, self.max_out])
        self.x = self.sigmoid(self.x)
        return self.x

class HybridAttention:
    def __init__(self, x,use_skip_connection=False):
        self.x=x
        self.ca = ZeroChannelAttention()
        self.sa = ZeroSpatialAttention()
        self.use_skip_connection = use_skip_connection

    def forward(self):
        out = self.x
        out = out + out * self.sa.forward(out) if self.use_skip_connection else out * self.sa.forward(out)
        out =out[:self.x.shape[0], :self.x.shape[1], :self.x.shape[2], :self.x.shape[3]]
        ## Hybrid
        out=np.array(out)
        out=position_attention(out)
        return out

def Hybrid_Attention_Block(w):
    import tensorflow as tf
    x = tf.convert_to_tensor(w)
    feature = HybridAttention(x).forward()
    newweight = tf.reshape(feature, [w.shape[0], w.shape[1], w.shape[2],w.shape[3]])
    newweight = np.array(newweight)
    return newweight

def Hybrid_Attention_Module(model):
    ###it will get thew weights frm module
    weights = model.get_weights()
    weight = weights[0]
    tunedweight = Hybrid_Attention_Block(weight)
    weights[0]=tunedweight
    model.set_weights(weights)
    return model

# Define your CNN model
def Hybrid_Attention_Based_Explainable_CNN(xtrain,ytrain,xtest, ytest,epochs,option):
    xtrain = xtrain.reshape(xtrain.shape[0], xtrain.shape[1] // 2, 2, 1)
    xtest = xtest.reshape(xtest.shape[0], xtest.shape[1] // 2, 2, 1)
    model = keras.Sequential()
    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding="same",input_shape = xtrain.shape[1:]))
    model.add(MaxPooling2D(pool_size=(1,1)))
    model.add(BatchNormalization())

    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding="same"))
    model.add(MaxPooling2D(pool_size=(1,1)))
    model.add(BatchNormalization())

    model.add(Conv2D(96, kernel_size=(3, 3), activation='relu', padding="same"))
    model.add(MaxPooling2D(pool_size=(1,1)))
    model.add(BatchNormalization())

    model.add(Conv2D(96, kernel_size=(3, 3), activation='relu', padding="same"))
    model.add(MaxPooling2D(pool_size=(1,1)))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))

    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding="same"))
    model = Hybrid_Attention_Module(model)
    model.add(MaxPooling2D(pool_size=(1,1)))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))

    model.add(Flatten())
    model.add(Dense(256, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.3))
    model.add(Dense(ytrain.shape[1], activation='softmax')) ## No of classes
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    print('\033[46m' + '\033[30m' + "________________________Hybrid Attention Based Explainable CNN Model prepared...__________________________________" + '\x1b[0m')

    model.fit(xtrain, ytrain, batch_size=32, epochs=epochs, validation_split=0.2)
    if option == 0:
        model = model
    else:
        op = Optimization(model, xtest, ytest)
        model = op.main_update_hyperparameters(option)

    Y_pred = model.predict(xtest)
    Y_pred = np.argmax(Y_pred, axis=1)
    Y_true = np.argmax(ytest, axis=1)
    return Y_pred,Y_true

class Optimization:
    def __init__(self, model, x_test, y_test):
        self.model = model
        self.x_test = x_test
        self.y_test = y_test

    def fitness_function1(self, solution):
        print(colored("Fitness Function >> ", color='blue', on_color='on_grey'))
        wei_to_train = self.model.get_weights()
        wei_sh = wei_to_train[0]
        wei = solution.reshape(wei_sh.shape[0], wei_sh.shape[1], wei_sh.shape[2], wei_sh.shape[3])
        wei_to_train[0] = wei
        self.model.set_weights(wei_to_train)
        preds = self.model.predict(self.x_test)
        preds = np.argmax(preds, axis=1)
        y1_test = np.argmax(self.y_test, axis=1)
        acc = accuracy_score(y1_test, preds)
        return acc

    def main_weight_updation_optimization(self, curr_wei,option):
        problem_dict1 = {
            "fit_func": self.fitness_function1,
            "lb": [curr_wei.min(), ] * curr_wei.shape[0] * curr_wei.shape[1],
            "ub": [curr_wei.max(), ] * curr_wei.shape[0] * curr_wei.shape[1],
            "minmax": "max",
            "log_to": None,
            "save_population": False,
            "Curr_Weight": curr_wei,
        }
        if option==1:
            print((colored("[INFO] Coyote Optimization \U0001F43A", 'magenta', on_color='on_grey')))
            model = COA(problem_dict1, epoch=2, pop_size=10)
            best_position2, best_fitness2 = model.solve()
        elif option==2:
            print((colored("[INFO] Grey Wolf Optimization \U0001F43A", 'magenta', on_color='on_grey')))
            model = GWO(problem_dict1, epoch=2, pop_size=10)
            best_position2, best_fitness2 = model.solve()
        else:
            print((colored("[INFO] Proposed Optimization \U0001F43A", 'magenta', on_color='on_grey')))
            model = Prop(problem_dict1, epoch=2, pop_size=10)
            best_position2, best_fitness2 = model.solve()
        return best_position2

    def main_update_hyperparameters(self, option):
        wei_to_train = self.model.get_weights()
        to_opt_1 = wei_to_train[0]
        re_to_opt_1 = to_opt_1.reshape(to_opt_1.shape[0] * to_opt_1.shape[1], to_opt_1.shape[2] * to_opt_1.shape[3])
        wei_to_train_1 = self.main_weight_updation_optimization(re_to_opt_1, option)
        to_opt_new = wei_to_train_1.reshape(to_opt_1.shape[0], to_opt_1.shape[1], to_opt_1.shape[2], to_opt_1.shape[3])
        wei_to_train[0] = to_opt_new
        self.model.set_weights(wei_to_train)
        return self.model

def main_est_perf_metrics(preds, y_test):

    confusion_mtx = confusion_matrix(y_test, preds)
    #cm =sum(mcm)
    """Total Counts of Confusion Matrix"""
    total = sum(sum(confusion_mtx))
    # """True Positive"""
    TP = confusion_mtx[0, 0]
    """False Positive"""
    FP = confusion_mtx[0, 1]
    """False Negative"""
    FN = confusion_mtx[1, 0]
    """True Negative"""
    TN = confusion_mtx[1, 1]
    """Accuracy Formula"""
    acc = (TP + TN) / total
    """Sensitivity Formula"""
    sen = TP / (FN + TP)
    """Specificity Formula"""
    spe = TN / (FP + TN)
    """Precision Formula"""
    pre = TP / (TP + FP)
    """Recall Formula"""
    rec = TP / (FN + TP)
    """F1 Score Formula"""
    f1_score = (2 * pre * rec) / (pre + rec)
    '''Critical Success Index '''
    CSI = TP/(TP+FN+FP)
    '''False Positivie Rate'''
    FPR = FP / (FP + TN)  # 1 - Specificity
    '''False Negative Rate'''
    FNR = FN / (TP + FN)  # 1 - Sensitivity
    '''Matthews Correlation Coefficient'''
    MCC = (TP * TN - FP * FN) / np.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))
    '''Negative Predictive Value'''
    NPV = TN / (TN + FN)  # negative predictive value
    '''Positive Predictive Value'''
    PPV = TP / (TP + FP)  # Positive Predictive Value

    return [acc, sen, spe, pre,rec,f1_score,CSI,FPR,FNR,MCC,PPV,NPV]

def oversample(X_train, y_train):
    ros = RandomOverSampler(random_state=42)
    X_resampled, y_resampled = ros.fit_resample(X_train, y_train)
    return X_resampled, y_resampled

def Explainable(image):
    print("Explainable CNN")
    image = cv2.cvtColor(np.uint8(image), cv2.COLOR_GRAY2BGR)## convert it into color
    Final = Guided_Propagation_Model(image, CAM_Model=GradCAMPlusPlus) ## GradCam++ alogrithm
    # Reshape Final to have 2154 rows and 3 columns
    Final_reshaped = Final.reshape(1, 2154, 3)

    # Check the shape of the reshaped array
    print(Final_reshaped.shape)  # Output: (1, 2154, 3)

    return Final_reshaped
    return Final

def Explainable_CNN_features(feat,option):
    print("Explainable CNN")
    if option==True:
        x_feat = []
        for i in range(feat.shape[0]):
            print(i)
            x_feat.append(Explainable(feat[i]))
        x_feat = np.vstack(x_feat)
    else:
        x_feat=np.load("Features/final_features.npy")
    return x_feat

def TP_Analysis(features,labels):
    print("Explainable CNN")
    features = Explainable_CNN_features(features, False)
    le = LabelEncoder()

    labels = le.fit_transform(labels)

    features, labels = oversample(features, labels)

    epochs = [20, 40, 60, 80, 100]  # No. of Iterations
    # epochs = [2, 4, 6, 8, 10, 2]

    tr = [0.4, 0.5, 0.6, 0.7, 0.8]  # Variation of Training Percentage - takes datasets rom smaller percentage to higher percentage to give the training percentage
    # tr = [0.7, 0.8]
    # 0 - hYBRID attENTION X CNN ,!-COA +   hYBRID attENTION X CNN , 2-GWO +  hYBRID attENTION X CNN,3 - Proposed  hYBRID attENTION X CNN
    options = [0, 1, 2, 3]

    COM_A = []
    COM_B = []
    COM_C = []
    COM_D = []
    COM_E = []
    COM_F = []
    COM_G = []
    COM_H = []
    COM_I = []
    COM_J = []
    COM_K = []
    COM_L = []

    for p in range(len(tr)):
        print(  '\033[46m' + '\033[30m' + "Training Percentage and Testing Percentage : " + str(tr[p] * 100) + " and " + str(
                100 - (tr[p] * 100)) + '\x1b[0m')

        xtrain, xtest, ytrain, ytest = split_data(features, labels, train_size=tr[p])

        y1train = to_categorical(ytrain)
        y1test = to_categorical(ytest)

        print('\033[46m' + '\033[30m' + "------------------------------MODEL TRAINING SECTION---------------------------------------"  + '\x1b[0m')
        Y_pred1, Y_true1 = LSTM_Model(xtrain,y1train,xtest,y1test, epochs[0])
        Y_pred2, Y_true2 = Bi_LSTM_MODEL(xtrain,y1train,xtest,y1test, epochs[0])
        Y_pred3, Y_true3 = CNN_Based_On_ResNet(xtrain,y1train,xtest,y1test, epochs[0])
        Y_pred4, Y_true4 = Adaptive_Attention_Fusion_Mechanism_CNN(xtrain,y1train,xtest,y1test,epochs[0])
        Y_pred5, Y_true5 = Hybrid_Attention_Based_Explainable_CNN(xtrain, y1train, xtest, y1test, epochs[4],options[0])
        Y_pred6, Y_true6 = Hybrid_Attention_Based_Explainable_CNN(xtrain, y1train, xtest, y1test, epochs[4], options[1])
        Y_pred7, Y_true7 = Hybrid_Attention_Based_Explainable_CNN(xtrain, y1train, xtest, y1test, epochs[4], options[2])
        Y_pred8, Y_true8 = Hybrid_Attention_Based_Explainable_CNN(xtrain,y1train,xtest,y1test,epochs[0], options[3])
        Y_pred9, Y_true9 = Hybrid_Attention_Based_Explainable_CNN(xtrain, y1train, xtest, y1test, epochs[1], options[3])
        Y_pred10, Y_true10 = Hybrid_Attention_Based_Explainable_CNN(xtrain, y1train, xtest, y1test, epochs[2], options[3])
        Y_pred11, Y_true11 = Hybrid_Attention_Based_Explainable_CNN(xtrain, y1train, xtest, y1test, epochs[3], options[3])
        Y_pred12, Y_true12 = Hybrid_Attention_Based_Explainable_CNN(xtrain, y1train, xtest, y1test, epochs[4], options[3])

        print('\033[46m' + '\033[30m' + "________________________Metrics Evaluated from Confusion Matrix__________________________________" + '\x1b[0m')

        [ACC1, SEN1, SPE1, PRE1, REC1, FSC1, CSI1, FPR1, FNR1, MCC1, PPV1, NPV1] = main_est_perf_metrics(Y_pred1,Y_true1)
        [ACC2, SEN2, SPE2, PRE2, REC2, FSC2, CSI2, FPR2, FNR2, MCC2, PPV2, NPV2] = main_est_perf_metrics(Y_pred2, Y_true2)
        [ACC3, SEN3, SPE3, PRE3, REC3, FSC3, CSI3, FPR3, FNR3, MCC3, PPV3, NPV3] = main_est_perf_metrics(Y_pred3, Y_true3)
        [ACC4, SEN4, SPE4, PRE4, REC4, FSC4, CSI4, FPR4, FNR4, MCC4, PPV4, NPV4] = main_est_perf_metrics(Y_pred4, Y_true4 )
        [ACC5, SEN5, SPE5, PRE5, REC5, FSC5, CSI5, FPR5, FNR5, MCC5, PPV5, NPV5] = main_est_perf_metrics(Y_pred5, Y_true5)
        [ACC6, SEN6, SPE6, PRE6, REC6, FSC6, CSI6, FPR6, FNR6, MCC6, PPV6, NPV6] = main_est_perf_metrics(Y_pred6, Y_true6)
        [ACC7, SEN7, SPE7, PRE7, REC7, FSC7, CSI7, FPR7, FNR7, MCC7, PPV7, NPV7] = main_est_perf_metrics(Y_pred7, Y_true7)
        [ACC8, SEN8, SPE8, PRE8, REC8, FSC8, CSI8, FPR8, FNR8, MCC8, PPV8, NPV8] = main_est_perf_metrics(Y_pred8, Y_true8)
        [ACC9, SEN9, SPE9, PRE9, REC9, FSC9, CSI9, FPR9, FNR9, MCC9, PPV9, NPV9] = main_est_perf_metrics(Y_pred9, Y_true9)
        [ACC10, SEN10, SPE10, PRE10, REC10, FSC10, CSI10, FPR10, FNR10, MCC10, PPV10, NPV10] = main_est_perf_metrics(Y_pred10,Y_true10)
        [ACC11, SEN11, SPE11, PRE11, REC11, FSC11, CSI11, FPR11, FNR11, MCC11, PPV11, NPV11] = main_est_perf_metrics(Y_pred11, Y_true11)
        [ACC12, SEN12, SPE12, PRE12, REC12, FSC12, CSI12, FPR12, FNR12, MCC12, PPV12, NPV12] = main_est_perf_metrics( Y_pred12, Y_true12)

        print('\033[46m' + '\033[30m' +"________________________Save Metrics__________________________________" + '\x1b[0m')

        COM_A.append([ACC1, SEN1, SPE1, PRE1, REC1, FSC1, CSI1, FPR1, FNR1, MCC1, PPV1, NPV1])
        COM_B.append([ACC2, SEN2, SPE2, PRE2, REC2, FSC2, CSI2, FPR2, FNR2, MCC2, PPV2, NPV2])
        COM_C.append([ACC3, SEN3, SPE3, PRE3, REC3, FSC3, CSI3, FPR3, FNR3, MCC3, PPV3, NPV3])
        COM_D.append([ACC4, SEN4, SPE4, PRE4, REC4, FSC4, CSI4, FPR4, FNR4, MCC4, PPV4, NPV4])
        COM_E.append([ACC5, SEN5, SPE5, PRE5, REC5, FSC5, CSI5, FPR5, FNR5, MCC5, PPV5, NPV5])
        COM_F.append([ACC6, SEN6, SPE6, PRE6, REC6, FSC6, CSI6, FPR6, FNR6, MCC6, PPV6, NPV6])
        COM_G.append([ACC7, SEN7, SPE7, PRE7, REC7, FSC7, CSI7, FPR7, FNR7, MCC7, PPV7, NPV7])
        COM_H.append([ACC8, SEN8, SPE8, PRE8, REC8, FSC8, CSI8, FPR8, FNR8, MCC8, PPV8, NPV8])
        COM_I.append([ACC9, SEN9, SPE9, PRE9, REC9, FSC9, CSI9, FPR9, FNR9, MCC9, PPV9, NPV9])
        COM_J.append([ACC10, SEN10, SPE10, PRE10, REC10, FSC10, CSI10, FPR10, FNR10, MCC10, PPV10, NPV10])
        COM_K.append([ACC11, SEN11, SPE11, PRE11, REC11, FSC11, CSI11, FPR11, FNR11, MCC11, PPV11, NPV11])
        COM_L.append([ACC12, SEN12, SPE12, PRE12, REC12, FSC12, CSI12, FPR12, FNR12, MCC12, PPV12, NPV12])

    np.save('NPY\\COM_A.npy'.format(os.getcwd()), COM_A)
    np.save('NPY\\COM_B.npy'.format(os.getcwd()), COM_B)
    np.save('NPY\\COM_C.npy'.format(os.getcwd()), COM_C)
    np.save('NPY\\COM_D.npy'.format(os.getcwd()), COM_D)
    np.save('NPY\\COM_E.npy'.format(os.getcwd()), COM_E)
    np.save('NPY\\COM_F.npy'.format(os.getcwd()), COM_F)
    np.save('NPY\\COM_G.npy'.format(os.getcwd()), COM_G)
    np.save('NPY\\COM_H.npy'.format(os.getcwd()), COM_H)
    np.save('NPY\\COM_I.npy'.format(os.getcwd()), COM_I)
    np.save('NPY\\COM_J.npy'.format(os.getcwd()), COM_J)
    np.save('NPY\\COM_K.npy'.format(os.getcwd()), COM_K)
    np.save('NPY\\COM_L.npy'.format(os.getcwd()), COM_L)

def parameter(acc, sen, spe, pre, rec, fsc,csi,fpr,fnr,mcc,ppv,npv):
    acc = np.mean(acc)
    sen = np.mean(sen)
    spe = np.mean(spe)
    pre = np.mean(pre)
    rec = np.mean(rec)
    fsc = np.mean(fsc)
    csi=np.mean(csi)
    fpr=np.mean(fpr)
    fnr=np.mean(fnr)
    mcc=np.mean(mcc)
    ppv=np.mean(ppv)
    npv=np.mean(npv)
    return [acc, sen, spe, pre, rec, fsc,csi,fpr,fnr,mcc,ppv,npv]

def  KF_Analysis(feat,lab):

    features = Explainable_CNN_features(feat, True)
    le = LabelEncoder()


    labels = le.fit_transform(lab)
    features, labels = oversample(features, labels)
    # features, labels = oversample(features, labels)
    """K-fold cross-validation approach divides the input dataset into K groups of samples of equal sizes.
     These samples are called folds. For each learning set, the prediction function uses k-Cotton Leaf folds,
      and the rest of the folds are used for the test set. This approach is a very popular CV approach
      because it is easy to understand, and the output is less biased than other methods."""
    kr = [4, 6, 8, 10]
    """An epoch in machine learning means one complete pass of the training dataset through the algorithm.
         This epoch's number is an important hyperparameter for the algorithm. 
         It specifies the number of epochs or complete passes of the entire training dataset passing through the training or learning process of the algorithm"""
    # epochs =[2,3,4,5,1] # No. of Iterations
    epochs = [10, 20, 30, 40, 50]
    """A label represents an output value,
       while a feature is an input value that describes the characteristics of such labels in datasets"""
    features = np.nan_to_num(features, 0)
    options = [0, 1, 2, 3]
    """Normalization is a technique often applied as part of data preparation for machine learning. 
        The goal of normalization is to change the values of numeric columns in the dataset to use a common scale,
         without distorting differences in the ranges of values or losing information"""
    ## Normalization
    feat = features.astype(np.float32) / features.max()

    COM_A = []
    COM_B = []
    COM_C = []
    COM_D = []
    COM_E = []
    COM_F = []
    COM_G = []
    COM_H = []

    """The steps for k-fold cross-validation are:
            Split the input dataset into K groups
        For each group:
        Take one group as the reserve or test data set.
        Use remaining groups as the training dataset
        Fit the model on the training set and evaluate the performance of the model using the test set."""
    for w in range(len(kr)):
        print(kr[w])
        strtfdKFold = StratifiedKFold(n_splits=kr[w])
        kfold = strtfdKFold.split(feat, labels)
        acc1, sen1, spe1, pre1, rec1, fsc1,csi1,fpr1,fnr1,mcc1,ppv1,npv1 = [], [], [], [], [], [], [], [], [], [], [], []
        acc2, sen2, spe2, pre2, rec2, fsc2,csi2,fpr2,fnr2,mcc2,ppv2,npv2 = [], [], [], [], [], [], [], [], [], [], [], []
        acc3, sen3, spe3, pre3, rec3, fsc3,csi3,fpr3,fnr3,mcc3,ppv3,npv3 = [], [], [], [], [], [], [], [], [], [], [], []
        acc4, sen4, spe4, pre4, rec4, fsc4,csi4,fpr4,fnr4,mcc4,ppv4,npv4 = [], [], [], [], [], [], [], [], [], [], [], []
        acc5, sen5, spe5, pre5, rec5, fsc5,csi5,fpr5,fnr5,mcc5,ppv5,npv5= [], [], [], [], [], [], [], [], [], [], [], []
        acc6, sen6, spe6, pre6, rec6, fsc6,csi6,fpr6,fnr6,mcc6,ppv6,npv6 = [], [], [], [], [], [], [], [], [], [], [], []
        acc7, sen7, spe7, pre7, rec7, fsc7,csi7,fpr7,fnr7,mcc7,ppv7,npv7 = [], [], [], [], [], [], [], [], [], [], [], []
        acc8, sen8, spe8, pre8, rec8, fsc8,csi8,fpr8,fnr8 ,mcc8,ppv8,npv8= [], [], [], [], [], [], [], [], [], [], [], []

        """Let's take an example of 5-folds cross-validation. So, the dataset is grouped into 5 folds.
            On 1st iteration, the first fold is reserved for test the model, and rest are used to train the model.
             On 2nd iteration, the second fold is used to test the model, and rest are used to train the model. 
             This process will continue until each fold is not used for the test fold."""

        for k, (train, test) in enumerate(kfold):
            if k==0:
                tr_data = feat[train, :]
                tr_data = tr_data[:, :]
                ytrain = labels[train]
                tst_data = feat[test, :]
                tst_data = tst_data[:, :]
                ytest = labels[test]
                xtrain = tr_data
                xtest = tst_data

                y1train = keras.utils.to_categorical(ytrain)
                y1test = keras.utils.to_categorical(ytest)

                print("------------------------------MODEL TRAINING SECTION---------------------------------------")

                Y_pred1, Y_true1 = LSTM_Model(xtrain, y1train, xtest, y1test, epochs[0])
                Y_pred2, Y_true2 = Bi_LSTM_MODEL(xtrain, y1train, xtest, y1test, epochs[0])
                Y_pred3, Y_true3 = CNN_Based_On_ResNet(xtrain, y1train, xtest, y1test, epochs[0])
                Y_pred4, Y_true4 = Adaptive_Attention_Fusion_Mechanism_CNN(xtrain, y1train, xtest, y1test, epochs[0])
                Y_pred5, Y_true5 = Hybrid_Attention_Based_Explainable_CNN(xtrain, y1train, xtest, y1test, epochs[4],
                                                                          options[0])
                Y_pred6, Y_true6 = Hybrid_Attention_Based_Explainable_CNN(xtrain, y1train, xtest, y1test, epochs[4],
                                                                          options[1])
                Y_pred7, Y_true7 = Hybrid_Attention_Based_Explainable_CNN(xtrain, y1train, xtest, y1test, epochs[4],
                                                                          options[2])
                Y_pred8, Y_true8 = Hybrid_Attention_Based_Explainable_CNN(xtrain, y1train, xtest, y1test, epochs[4],
                                                                          options[3])

                print("________________________Metrics Evaluated from Confusion Matrix__________________________________")
                [ACC1, SEN1, SPE1, PRE1, REC1, FSC1, CSI1, FPR1, FNR1,MCC1,PPV1,NPV1] = main_est_perf_metrics(Y_pred1, Y_true1)
                [ACC2, SEN2, SPE2, PRE2, REC2, FSC2, CSI2, FPR2, FNR2,MCC2,PPV2,NPV2] = main_est_perf_metrics(Y_pred2, Y_true2)
                [ACC3, SEN3, SPE3, PRE3, REC3, FSC3, CSI3, FPR3, FNR3,MCC3,PPV3,NPV3] = main_est_perf_metrics(Y_pred3, Y_true3)
                [ACC4, SEN4, SPE4, PRE4, REC4, FSC4, CSI4, FPR4, FNR4,MCC4,PPV4,NPV4] = main_est_perf_metrics(Y_pred4, Y_true4)
                [ACC5, SEN5, SPE5, PRE5, REC5, FSC5, CSI5, FPR5, FNR5,MCC5,PPV5,NPV5] = main_est_perf_metrics(Y_pred5, Y_true5)
                [ACC6, SEN6, SPE6, PRE6, REC6, FSC6, CSI6, FPR6, FNR6,MCC6,PPV6,NPV6] = main_est_perf_metrics(Y_pred6, Y_true6)
                [ACC7, SEN7, SPE7, PRE7, REC7, FSC7, CSI7, FPR7, FNR7,MCC7,PPV7,NPV7] = main_est_perf_metrics(Y_pred7, Y_true7)
                [ACC8, SEN8, SPE8, PRE8, REC8, FSC8, CSI8, FPR8, FNR8,MCC8,PPV8,NPV8] = main_est_perf_metrics(Y_pred8, Y_true8)

                acc1.append(ACC1)
                sen1.append(SEN1)
                spe1.append(SPE1)
                pre1.append(PRE1)
                rec1.append(REC1)
                fsc1.append(FSC1)
                acc2.append(ACC2)
                sen2.append(SEN2)
                spe2.append(SPE2)
                pre2.append(PRE2)
                rec2.append(REC2)
                fsc2.append(FSC2)
                acc3.append(ACC3)
                sen3.append(SEN3)
                spe3.append(SPE3)
                pre3.append(PRE3)
                rec3.append(REC3)
                fsc3.append(FSC3)
                acc4.append(ACC4)
                sen4.append(SEN4)
                spe4.append(SPE4)
                pre4.append(PRE4)
                rec4.append(REC4)
                fsc4.append(FSC4)
                acc5.append(ACC5)
                sen5.append(SEN5)
                spe5.append(SPE5)
                pre5.append(PRE5)
                rec5.append(REC5)
                fsc5.append(FSC5)
                acc6.append(ACC6)
                sen6.append(SEN6)
                spe6.append(SPE6)
                pre6.append(PRE6)
                rec6.append(REC6)
                fsc6.append(FSC6)
                acc7.append(ACC7)
                sen7.append(SEN7)
                spe7.append(SPE7)
                pre7.append(PRE7)
                rec7.append(REC7)
                fsc7.append(FSC7)
                acc8.append(ACC8)
                sen8.append(SEN8)
                spe8.append(SPE8)
                pre8.append(PRE8)
                rec8.append(REC8)
                fsc8.append(FSC8)
                csi1.append(CSI1)
                fpr1.append(FPR1)
                fnr1.append(FNR1)
                csi2.append(CSI2)
                fpr2.append(FPR2)
                fnr2.append(FNR2)
                csi3.append(CSI3)
                fpr3.append(FPR3)
                fnr3.append(FNR3)
                csi4.append(CSI4)
                fpr4.append(FPR4)
                fnr4.append(FNR4)
                csi5.append(CSI5)
                fpr5.append(FPR5)
                fnr5.append(FNR5)
                csi6.append(CSI6)
                fpr6.append(FPR6)
                fnr6.append(FNR6)
                csi7.append(CSI7)
                fpr7.append(FPR7)
                fnr7.append(FNR7)
                csi8.append(CSI8)
                fpr8.append(FPR8)
                fnr8.append(FNR8)
                mcc1.append(MCC1)
                ppv1.append(PPV1)
                npv1.append(NPV1)
                mcc2.append(MCC2)
                ppv2.append(PPV2)
                npv2.append(NPV2)
                mcc3.append(MCC3)
                ppv3.append(PPV3)
                npv3.append(NPV3)
                mcc4.append(MCC4)
                ppv4.append(PPV4)
                npv4.append(NPV4)
                mcc5.append(MCC5)
                ppv5.append(PPV5)
                npv5.append(NPV5)
                mcc6.append(MCC6)
                ppv6.append(PPV6)
                npv6.append(NPV6)
                mcc7.append(MCC7)
                ppv7.append(PPV7)
                npv7.append(NPV7)
                mcc8.append(MCC8)
                ppv8.append(PPV8)
                npv8.append(NPV8)

        [ACC_1, SEN_1, SPE_1, PRE_1, REC_1, FSC_1,CSI_1,FPR_1,FNR_1,MCC_1,PPV_1,NPV_1] = parameter(acc1, sen1, spe1, pre1, rec1, fsc1,csi1,fpr1,fnr1,mcc1,ppv1,npv1)
        [ACC_2, SEN_2, SPE_2, PRE_2, REC_2, FSC_2,CSI_2,FPR_2,FNR_2,MCC_2,PPV_2,NPV_2] = parameter(acc2, sen2, spe2, pre2, rec2, fsc2,csi2,fpr2,fnr2,mcc2,ppv2,npv2)
        [ACC_3, SEN_3, SPE_3, PRE_3, REC_3, FSC_3,CSI_3,FPR_3,FNR_3,MCC_3,PPV_3,NPV_3] = parameter(acc3, sen3, spe3, pre3, rec3, fsc3,csi3,fpr3,fnr3,mcc3,ppv3,npv3)
        [ACC_4, SEN_4, SPE_4, PRE_4, REC_4, FSC_4,CSI_4,FPR_4,FNR_4,MCC_4,PPV_4,NPV_4] = parameter(acc4, sen4, spe4, pre4, rec4, fsc4,csi4,fpr4,fnr4,mcc4,ppv4,npv4)
        [ACC_5, SEN_5, SPE_5, PRE_5, REC_5, FSC_5,CSI_5,FPR_5,FNR_5,MCC_5,PPV_5,NPV_5] = parameter(acc5, sen5, spe5, pre5, rec5, fsc5,csi5,fpr5,fnr5,mcc5,ppv5,npv5)
        [ACC_6, SEN_6, SPE_6, PRE_6, REC_6, FSC_6,CSI_6,FPR_6,FNR_6,MCC_6,PPV_6,NPV_6] = parameter(acc6, sen6, spe6, pre6, rec6, fsc6,csi6,fpr6,fnr6,mcc6,ppv6,npv6)
        [ACC_7, SEN_7, SPE_7, PRE_7, REC_7, FSC_7,CSI_7,FPR_7,FNR_7,MCC_7,PPV_7,NPV_7] = parameter(acc7, sen7, spe7, pre7, rec7, fsc7,csi7,fpr7,fnr7,mcc7,ppv7,npv7)
        [ACC_8, SEN_8, SPE_8, PRE_8, REC_8, FSC_8,CSI_8,FPR_8,FNR_8,MCC_8,PPV_8,NPV_8] = parameter(acc8, sen8, spe8, pre8, rec8, fsc8,csi8,fpr8,fnr8,mcc8,ppv8,npv8)

        COM_A.append([ACC_1, SEN_1, SPE_1, PRE_1, REC_1, FSC_1,CSI_1,FPR_1,FNR_1,MCC_1,PPV_1,NPV_1])
        COM_B.append([ACC_2, SEN_2, SPE_2, PRE_2, REC_2, FSC_2,CSI_2,FPR_2,FNR_2,MCC_2,PPV_2,NPV_2])
        COM_C.append([ACC_3, SEN_3, SPE_3, PRE_3, REC_3, FSC_3,CSI_3,FPR_3,FNR_3,MCC_3,PPV_3,NPV_3])
        COM_D.append([ACC_4, SEN_4, SPE_4, PRE_4, REC_4, FSC_4,CSI_4,FPR_4,FNR_4,MCC_4,PPV_4,NPV_4])
        COM_E.append([ACC_5, SEN_5, SPE_5, PRE_5, REC_5, FSC_5,CSI_5,FPR_5,FNR_5,MCC_5,PPV_5,NPV_5])
        COM_F.append([ACC_6, SEN_6, SPE_6, PRE_6, REC_6, FSC_6,CSI_6,FPR_6,FNR_6,MCC_6,PPV_6,NPV_6])
        COM_G.append([ACC_7, SEN_7, SPE_7, PRE_7, REC_7, FSC_7,CSI_7,FPR_7,FNR_7,MCC_7,PPV_7,NPV_7])
        COM_H.append([ACC_8, SEN_8, SPE_8, PRE_8, REC_8, FSC_8,CSI_8,FPR_8,FNR_8,MCC_8,PPV_8,NPV_8])

    np.save('NPY1\\COM_A.npy'.format(os.getcwd()), COM_A)
    np.save('NPY1\\COM_B.npy'.format(os.getcwd()), COM_B)
    np.save('NPY1\\COM_C.npy'.format(os.getcwd()), COM_C)
    np.save('NPY1\\COM_D.npy'.format(os.getcwd()), COM_D)
    np.save('NPY1\\COM_E.npy'.format(os.getcwd()), COM_E)
    np.save('NPY1\\COM_F.npy'.format(os.getcwd()), COM_F)
    np.save('NPY1\\COM_G.npy'.format(os.getcwd()), COM_G)
    np.save('NPY1\\COM_H.npy'.format(os.getcwd()), COM_H)

def Complete_Figure_com(perf, val, str_1, xlab, ylab,name):
    perf = perf * 100
    a = perf[:, 0]
    b = perf[:, 1]
    c = perf[:, 2]
    d = perf[:, 3]
    e= perf[:,4]
    dict = {'40': a, '50': b, '60': c, '70': d,'80':e}
    df = pd.DataFrame(dict, index=[str_1])
    df.to_csv('Results_P1\\TP\\Comp_Analysis\\' + '_' + str(val) + str(name) +'_' + 'Graph.csv')
    df1 = {
        'No.of.records': ['40', '40', '40', '40', '40', '40', '40', '40',
                          '50', '50', '50', '50', '50', '50', '50', '50',
                          '60', '60', '60', '60', '60', '60', '60', '60',
                          '70', '70', '70', '70', '70', '70', '70', '70',
                          '80', '80', '80', '80', '80', '80', '80', '80'],
        'Accuracy(%)': [perf[0, 0], perf[1, 0], perf[2, 0], perf[3, 0], perf[4, 0], perf[5, 0], perf[6, 0], perf[7, 0]
            , perf[0, 1], perf[1, 1], perf[2, 1], perf[3, 1], perf[4, 1], perf[5, 1], perf[6, 1], perf[7, 1]
            , perf[0, 2], perf[1, 2], perf[2, 2], perf[3, 2], perf[4, 2], perf[5, 2], perf[6, 2], perf[7, 2]
            , perf[0, 3], perf[1, 3], perf[2, 3], perf[3, 3], perf[4, 3], perf[5, 3], perf[6, 3], perf[7, 3]
            , perf[0, 4], perf[1, 4], perf[2, 4], perf[3, 4], perf[4, 4], perf[5, 4], perf[6, 4], perf[7, 4]],
        'Legend': [str_1[0], str_1[1], str_1[2], str_1[3], str_1[4], str_1[5], str_1[6], str_1[7],
                   str_1[0], str_1[1], str_1[2], str_1[3], str_1[4], str_1[5], str_1[6], str_1[7],
                   str_1[0], str_1[1], str_1[2], str_1[3], str_1[4], str_1[5], str_1[6], str_1[7],
                   str_1[0], str_1[1], str_1[2], str_1[3], str_1[4], str_1[5], str_1[6], str_1[7],
                   str_1[0], str_1[1], str_1[2], str_1[3], str_1[4], str_1[5], str_1[6], str_1[7]]}
    plt.figure()
    sns.set_style("whitegrid")
    sns.set(font_scale=0.8)
    sns.barplot(x='No.of.records', y='Accuracy(%)', hue='Legend',palette=['#6600ff','#ff9900','#cc00ff','#99ff33','#1ab2ff','#ff6600','#669900','#cc3300'],data=df1)
    plt.legend(ncol=2,loc='lower center', fontsize=15,prop={"weight":"bold"})
    plt.xlabel(xlab, fontsize=15, weight='bold')
    plt.ylabel(ylab, fontsize=15, weight='bold')
    plt.savefig('Results_P1\\TP\\Comp_Analysis\\' + str(val) + '_' + str(name) +'Graph.png', dpi=800)
    plt.show(block=False)
    plt.clf()

def load_perf_value_saved_Algo_Analysis_1():
    perf_A = np.load('NPY\\COM_A.npy')
    perf_B = np.load('NPY\\COM_B.npy')
    perf_C = np.load('NPY\\COM_C.npy')
    perf_D = np.load('NPY\\COM_D.npy')
    perf_E = np.load('NPY\\COM_E.npy')
    perf_F = np.load('NPY\\COM_F.npy')
    perf_G = np.load('NPY\\COM_G.npy')
    perf_H = np.load('NPY\\COM_H.npy')
    perf_I = np.load('NPY\\COM_I.npy')
    perf_J = np.load('NPY\\COM_J.npy')
    perf_K = np.load('NPY\\COM_K.npy')
    perf_L = np.load('NPY\\COM_L.npy')

    A = np.asarray(perf_A[:][:])
    B = np.asarray(perf_B[:][:])
    C = np.asarray(perf_C[:][:])
    D = np.asarray(perf_D[:][:])
    E = np.asarray(perf_E[:][:])
    F = np.asarray(perf_F[:][:])
    G = np.asarray(perf_G[:][:])
    H = np.asarray(perf_H[:][:])
    I = np.asarray(perf_I[:][:])
    J = np.asarray(perf_J[:][:])
    K = np.asarray(perf_K[:][:])
    L = np.asarray(perf_L[:][:])

    AA = A[:][:].transpose()
    BB = B[:][:].transpose()
    CC = C[:][:].transpose()
    DD = D[:][:].transpose()
    EE = E[:][:].transpose()
    FF = F[:][:].transpose()
    GG = G[:][:].transpose()
    HH = H[:][:].transpose()
    II = I[:][:].transpose()
    JJ = J[:][:].transpose()
    KK = K[:][:].transpose()
    LL = L[:][:].transpose()

    return [AA, BB, CC, DD, EE, FF, GG, HH, II, JJ, KK, LL]

def load_perf_value_saved_Algo_Analysis_2():
    perf_A = np.load('NPY1\\COM_A.npy')
    perf_B = np.load('NPY1\\COM_B.npy')
    perf_C = np.load('NPY1\\COM_C.npy')
    perf_D = np.load('NPY1\\COM_D.npy')
    perf_E = np.load('NPY1\\COM_E.npy')
    perf_F = np.load('NPY1\\COM_F.npy')
    perf_G = np.load('NPY1\\COM_G.npy')
    perf_H = np.load('NPY1\\COM_H.npy')

    A = np.asarray(perf_A[:][:])
    B = np.asarray(perf_B[:][:])
    C = np.asarray(perf_C[:][:])
    D = np.asarray(perf_D[:][:])
    E = np.asarray(perf_E[:][:])
    F = np.asarray(perf_F[:][:])
    G = np.asarray(perf_G[:][:])
    H = np.asarray(perf_H[:][:])

    AA = A[:][:].transpose()
    BB = B[:][:].transpose()
    CC = C[:][:].transpose()
    DD = D[:][:].transpose()
    EE = E[:][:].transpose()
    FF = F[:][:].transpose()
    GG = G[:][:].transpose()
    HH = H[:][:].transpose()
    return [AA, BB, CC, DD, EE, FF, GG, HH]

def Main_comp_val_acc_sen_spe_1(AA,BB,CC,DD,EE,FF,GG,HH,II,JJ,KK,LL):
    VALLL = np.column_stack((AA[0], BB[0], CC[0], DD[0], EE[0], FF[0], GG[0], HH[0], II[0], JJ[0], KK[0], LL[0]))
    perf1 = VALLL.T
    # perf1 = vall(perf1)
    VALLL = np.column_stack((AA[1], BB[1], CC[1], DD[1], EE[1], FF[1], GG[1], HH[1], II[1], JJ[1], KK[1], LL[1]))
    perf2 = VALLL.T
    # perf2 = vall(perf2)
    VALLL = np.column_stack((AA[2], BB[2], CC[2], DD[2], EE[2], FF[2], GG[2], HH[2], II[2], JJ[2], KK[2], LL[2]))
    perf3 = VALLL.T
    # perf3 = vall(perf3)
    VALLL = np.column_stack((AA[3], BB[3], CC[3], DD[3], EE[3], FF[3], GG[3], HH[3], II[3], JJ[3], KK[3], LL[3]))
    perf4 = VALLL.T
    # perf4 = vall(perf4)
    VALLL = np.column_stack((AA[4], BB[4], CC[4], DD[4], EE[4], FF[4], GG[4], HH[4], II[4], JJ[4], KK[4], LL[4]))
    perf5 = VALLL.T
    # perf5 = vall(perf5)
    VALLL = np.column_stack((AA[5], BB[5], CC[5], DD[5], EE[5], FF[5], GG[5], HH[5], II[5], JJ[5], KK[5], LL[5]))
    perf6 = VALLL.T
    VALLL = np.column_stack((AA[6], BB[6], CC[6], DD[6], EE[6], FF[6], GG[6], HH[6], II[6], JJ[6], KK[6], LL[6]))
    perf7 = VALLL.T
    VALLL = np.column_stack((AA[7], BB[7], CC[7], DD[7], EE[7], FF[7], GG[7], HH[7], II[7], JJ[7], KK[7], LL[7]))
    perf8 = VALLL.T
    VALLL = np.column_stack((AA[8], BB[8], CC[8], DD[8], EE[8], FF[8], GG[8], HH[8], II[8], JJ[8], KK[8], LL[8]))
    perf9 = VALLL.T
    # perf6 = vall(perf6)
    # [perf1, perf2, perf3, perf4, perf5, perf6, perf7, perf8, perf9,perf10,perf11,perf12]=perfs(perf1, perf2, perf3, perf4, perf5, perf6,perf7,perf8,perf9,perf10,perf11,perf12,0)
    return [perf1, perf2, perf3, perf4, perf5, perf6,perf7,perf8,perf9]

def Main_comp_val_acc_sen_spe_2(AA,BB,CC,DD,EE,FF,GG,HH):
    VALLL = np.column_stack((AA[0], BB[0], CC[0], DD[0], EE[0], FF[0], GG[0], HH[0]))
    perf1 = VALLL.T
    # perf1 = vall(perf1)
    VALLL = np.column_stack((AA[1], BB[1], CC[1], DD[1], EE[1], FF[1], GG[1], HH[1]))
    perf2 = VALLL.T
    # perf2 = vall(perf2)
    VALLL = np.column_stack((AA[2], BB[2], CC[2], DD[2], EE[2], FF[2], GG[2], HH[2]))
    perf3 = VALLL.T
    # perf3 = vall(perf3)
    VALLL = np.column_stack((AA[3], BB[3], CC[3], DD[3], EE[3], FF[3], GG[3], HH[3]))
    perf4 = VALLL.T
    # perf4 = vall(perf4)
    VALLL = np.column_stack((AA[4], BB[4], CC[4], DD[4], EE[4], FF[4], GG[4], HH[4]))
    perf5 = VALLL.T
    # perf5 = vall(perf5)
    VALLL = np.column_stack((AA[5], BB[5], CC[5], DD[5], EE[5], FF[5], GG[5], HH[5]))
    perf6 = VALLL.T
    VALLL = np.column_stack((AA[6], BB[6], CC[6], DD[6], EE[6], FF[6], GG[6], HH[6]))
    perf7 = VALLL.T
    VALLL = np.column_stack((AA[7], BB[7], CC[7], DD[7], EE[7], FF[7], GG[7], HH[7]))
    perf8 = VALLL.T
    VALLL = np.column_stack((AA[8], BB[8], CC[8], DD[8], EE[8], FF[8], GG[8], HH[8]))
    perf9 = VALLL.T
    # perf6 = vall(perf6)
    # [perf1, perf2, perf3, perf4, perf5, perf6, perf7, perf8, perf9,perf10,perf11,perf12]=perfs(perf1, perf2, perf3, perf4, perf5, perf6,perf7,perf8,perf9,perf10,perf11,perf12)

    return [perf1, perf2, perf3, perf4, perf5, perf6,perf7,perf8,perf9]

def load_perf_parameter1(A, B, C, D, E, F,G,H,I):
    perf_A1 = A[[0, 1, 2, 3, 4, 5, 6, 11], :]
    perf_B1 = F[[0, 1, 2, 3, 4, 5, 6, 11], :]
    perf_C1 = D[[0, 1, 2, 3, 4, 5, 6, 11], :]
    perf_D1 = E[[0, 1, 2, 3, 4, 5, 6, 11], :]
    perf_A2 = A[[7, 8, 9, 10, 11], :]
    perf_B2 = F[[7, 8, 9, 10, 11], :]
    perf_C2 = D[[7, 8, 9, 10, 11], :]
    perf_D2 = E[[7, 8, 9, 10, 11], :]
    return [perf_A1, perf_B1, perf_C1, perf_D1,perf_A2, perf_B2, perf_C2,perf_D2]

def load_perf_parameter2(A, B, C, D, E, F,G,H,I):
    perf_A1 = A
    perf_B1 = F
    perf_C1 = D
    perf_D1 = E
    return [perf_A1, perf_B1, perf_C1, perf_D1]

def Complete_Figure_perf(perf, val, str_1, xlab, ylab,name):
    perf = perf * 100
    a = perf[:, 0]
    b = perf[:, 1]
    c = perf[:, 2]
    d = perf[:, 3]
    e = perf[:, 4]
    dict = {'40': a, '50': b, '60': c, '70': d, '80': e}
    df = pd.DataFrame(dict, index=[str_1])
    df.to_csv('Results_P1\\TP\\Perf_Analysis\\_' + str(val) + '_' + str(name) +'Graph.csv')
    df1 = {
        'No.of.records': ['40', '40', '40', '40', '40',
                          '50', '50', '50', '50', '50',
                          '60', '60', '60', '60', '60',
                          '70', '70', '70', '70', '70',
                          '80', '80', '80', '80', '80'],
        'Accuracy(%)': [perf[0, 0], perf[1, 0], perf[2, 0], perf[3, 0], perf[4, 0]
            , perf[0, 1], perf[1, 1], perf[2, 1], perf[3, 1], perf[4, 1]
            , perf[0, 2], perf[1, 2], perf[2, 2], perf[3, 2], perf[4, 2]
            , perf[0, 3], perf[1, 3], perf[2, 3], perf[3, 3], perf[4, 3]
            , perf[0, 4], perf[1, 4], perf[2, 4], perf[3, 4], perf[4, 4]],
        'Legend': [str_1[0], str_1[1], str_1[2], str_1[3], str_1[4],
                   str_1[0], str_1[1], str_1[2], str_1[3], str_1[4],
                   str_1[0], str_1[1], str_1[2], str_1[3], str_1[4],
                   str_1[0], str_1[1], str_1[2], str_1[3], str_1[4],
                   str_1[0], str_1[1], str_1[2], str_1[3], str_1[4]]}

    plt.figure()
    sns.set(font_scale=0.8)
    sns.set_style("whitegrid")
    sns.lineplot(x='No.of.records', y='Accuracy(%)', hue='Legend',  marker="d",markersize=10,palette=['#009900','#996600','#cc0000','#4da6ff','#6600ff'],data=df1)
    plt.legend(loc='lower center', fontsize=15,prop={'weight':'bold'})
    plt.xlabel(xlab, fontsize=15,weight='bold')
    plt.ylabel(ylab, fontsize=15,weight='bold')
    plt.savefig('Results_P1\\TP\\Perf_Analysis\\' + str(val) + '_' + str(name) +'Graph.png', dpi=800)
    plt.show(block=False)
    plt.clf()

def Complete_Figure_com_kf(perf, val, str_1, xlab, ylab,name):
    perf=perf*100
    a = perf[:, 0]
    b = perf[:, 1]
    c = perf[:, 2]
    d = perf[:, 3]
    dict = {'4': a, '6': b, '8': c, '10': d}
    df = pd.DataFrame(dict, index=[str_1])
    df.to_csv('Results_P1\\KF\\Comp_Analysis\\' + '_' + str(val) + str(name) +'_' + 'Graph.csv')
    df1 = {
        'No.of.records': ['4', '4', '4', '4', '4', '4', '4', '4',
                          '6', '6', '6', '6', '6', '6', '6', '6',
                          '8', '8', '8', '8', '8', '8', '8', '8',
                          '10', '10', '10', '10', '10', '10', '10', '10'],
        'Accuracy(%)': [perf[0, 0], perf[1, 0], perf[2, 0], perf[3, 0], perf[4, 0], perf[5, 0], perf[6, 0], perf[7, 0]
            , perf[0, 1], perf[1, 1], perf[2, 1], perf[3, 1], perf[4, 1], perf[5, 1], perf[6, 1], perf[7, 1]
            , perf[0, 2], perf[1, 2], perf[2, 2], perf[3, 2], perf[4, 2], perf[5, 2], perf[6, 2], perf[7, 2]
            , perf[0, 3], perf[1, 3], perf[2, 3], perf[3, 3], perf[4, 3], perf[5, 3], perf[6, 3], perf[7, 3]],
        'Legend': [str_1[0], str_1[1], str_1[2], str_1[3], str_1[4], str_1[5], str_1[6], str_1[7],
                   str_1[0], str_1[1], str_1[2], str_1[3], str_1[4], str_1[5], str_1[6], str_1[7],
                   str_1[0], str_1[1], str_1[2], str_1[3], str_1[4], str_1[5], str_1[6], str_1[7],
                   str_1[0], str_1[1], str_1[2], str_1[3], str_1[4], str_1[5], str_1[6], str_1[7],]}

    plt.figure()
    sns.set(font_scale=0.8)
    sns.set_style("whitegrid")
    sns.barplot(x='No.of.records', y='Accuracy(%)', hue='Legend', palette=['#6600ff','#ff9900','#cc00ff','#99ff33','#1ab2ff','#ff6600','#669900','#cc3300'],data=df1)
    plt.legend(ncol=2,loc='lower center', fontsize=15,prop={"weight":"bold"})
    plt.xlabel(xlab, fontsize=15, weight='bold')
    plt.ylabel(ylab, fontsize=15, weight='bold')
    plt.savefig('Results_P1\\KF\\Comp_Analysis\\' + str(val) + '_' + str(name) +'Graph.png', dpi=800)
    plt.show(block=False)
    plt.clf()

def complete_graph(ii):
    name = ["Accuracy", "F1-Score","Precision","Recall"]
    [AA, BB, CC, DD, EE, FF, GG, HH, II, JJ, KK, LL] = load_perf_value_saved_Algo_Analysis_1()
    [perf1, perf2, perf3, perf4, perf5, perf6,perf7,perf8,perf9] = Main_comp_val_acc_sen_spe_1(AA, BB, CC, DD, EE, FF, GG, HH, II, JJ, KK, LL)
    [perf_A1, perf_B1, perf_C1,perf_D1, perf_A2, perf_B2, perf_C2,perf_D2] = load_perf_parameter1(perf1, perf2, perf3, perf4, perf5,perf6,perf7,perf8,perf9)
    [AA, BB, CC, DD, EE, FF, GG, HH] = load_perf_value_saved_Algo_Analysis_2()
    [perf1, perf2, perf3, perf4, perf5, perf6,perf7,perf8,perf9] = Main_comp_val_acc_sen_spe_2(AA, BB, CC, DD, EE, FF, GG,HH)
    [perf_A,perf_B,perf_C,perf_D] = load_perf_parameter2(perf1, perf2, perf3, perf4,perf5,perf6,perf7,perf8,perf9)

    legend = ["LSTM", "BiLSTM", "ResNet-CNN", "AAFMCNN", "HA-Explainable CNN", "COA-HA-Explainable CNN","GWO-HA-Explainable CNN","Proposed HA-Explainable CNN"]
    legend1 = ["Proposed HA-Explainable CNN with Epochs=20","Proposed HA-Explainable CNN with Epochs=40","Proposed HA-Explainable CNN with Epochs=60",
               "Proposed HA-Explainable CNN with Epochs=80", "Proposed HA-Explainable CNN with Epochs=100"]
    xlab = "Training Percentage(%)"
    ylab = name[0]+"(%)"
    Complete_Figure_com(perf_A1, ii, legend, xlab, ylab, name[0])
    ii = ii + 1
    ylab =  name[1]+"(%)"
    Complete_Figure_com(perf_B1, ii, legend, xlab, ylab, name[1])
    ii = ii + 1
    ylab = name[2]+"(%)"
    Complete_Figure_com(perf_C1, ii, legend, xlab, ylab, name[2])
    ii = ii + 1
    ylab =  name[3]+"(%)"
    Complete_Figure_com(perf_D1, ii, legend, xlab, ylab, name[3])
    ii = ii + 1
    ylab =   name[0]+"(%)"
    Complete_Figure_perf(perf_A2, ii, legend1, xlab, ylab, name[0])
    ii = ii + 1
    ylab =  name[1]+"(%)"
    Complete_Figure_perf(perf_B2, ii, legend1, xlab, ylab, name[1])
    ii = ii + 1
    ylab =  name[2]+"(%)"
    Complete_Figure_perf(perf_C2, ii, legend1, xlab, ylab, name[2])
    ii = ii + 1
    ylab =  name[3]+"(%)"
    Complete_Figure_perf(perf_D2, ii, legend1, xlab, ylab, name[3])
    ii = ii + 1

    xlab = "K-Fold"
    ylab =   name[0]+"(%)"
    Complete_Figure_com_kf(perf_A, ii, legend, xlab, ylab, name[0])
    ii = ii + 1
    ylab =  name[1]+"(%)"
    Complete_Figure_com_kf(perf_B, ii, legend, xlab, ylab, name[1])
    ii = ii + 1
    ylab =  name[2]+"(%)"
    Complete_Figure_com_kf(perf_C, ii, legend, xlab, ylab, name[2])
    ii = ii + 1
    ylab =  name[3]+"(%)"
    Complete_Figure_com_kf(perf_D, ii, legend, xlab, ylab, name[3])

def All_Analysis():
    print('\33[44m' + '\33[31m' + "[INFO] Loading Features and Labels" + '\x1b[0m')
    feat = np.load("Final Featres/final_features.npy")[:100]
    lab = np.load("Final Featres/Labels.npy")
    print('\33[46m' + '\33[30m' + "--------------[INFO] Training Percentage Analysis------------- " + '\x1b[0m')
    TP_Analysis(feat,lab)
    print('\33[46m' + '\33[30m' + "-------------[INFO] Cross Validation Analysis-------------- " + '\x1b[0m')
    KF_Analysis(feat,lab)








